{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <img style=\"float: left; padding-right: 10px; width: 45px\" src=\"https://github.com/Harvard-IACS/2021-s109a/blob/master/lectures/crest.png?raw=true\"> CS-S109A Introduction to Data Science\n",
    "\n",
    "\n",
    "## Lab 4: Principle Component Analysis (PCA), Logistcic Regression and KNN Classification\n",
    "\n",
    "**Harvard University**<br>\n",
    "**Summer 2021**<br>\n",
    "**Contributors:** Will Claybaugh, David Sondak, Chris Tanner, Arpit Panda, Kevin Rader, Shivam Raval\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "blockquote { background: #AEDE94; }\n",
       "h1 { \n",
       "    padding-top: 25px;\n",
       "    padding-bottom: 25px;\n",
       "    text-align: left; \n",
       "    padding-left: 10px;\n",
       "    background-color: #DDDDDD; \n",
       "    color: black;\n",
       "}\n",
       "h2 { \n",
       "    padding-top: 10px;\n",
       "    padding-bottom: 10px;\n",
       "    text-align: left; \n",
       "    padding-left: 5px;\n",
       "    background-color: #EEEEEE; \n",
       "    color: black;\n",
       "}\n",
       "\n",
       "div.exercise {\n",
       "\tbackground-color: #ffcccc;\n",
       "\tborder-color: #E9967A; \t\n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "}\n",
       "\n",
       "div.exercise-r {\n",
       "\tbackground-color: #fce8e8;\n",
       "\tborder-color: #E9967A; \t\n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "}\n",
       "\n",
       "\n",
       "span.sub-q {\n",
       "\tfont-weight: bold;\n",
       "}\n",
       "div.theme {\n",
       "\tbackground-color: #DDDDDD;\n",
       "\tborder-color: #E9967A; \t\n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "\tfont-size: 18pt;\n",
       "}\n",
       "div.gc { \n",
       "\tbackground-color: #AEDE94;\n",
       "\tborder-color: #E9967A; \t \n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "\tfont-size: 12pt;\n",
       "}\n",
       "p.q1 { \n",
       "    padding-top: 5px;\n",
       "    padding-bottom: 5px;\n",
       "    text-align: left; \n",
       "    padding-left: 5px;\n",
       "    background-color: #EEEEEE; \n",
       "    color: black;\n",
       "}\n",
       "header {\n",
       "   padding-top: 35px;\n",
       "    padding-bottom: 35px;\n",
       "    text-align: left; \n",
       "    padding-left: 10px;\n",
       "    background-color: #DDDDDD; \n",
       "    color: black;\n",
       "}\n",
       "</style>\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## RUN THIS CELL TO PROPERLY HIGHLIGHT THE EXERCISES\n",
    "import requests\n",
    "from IPython.core.display import HTML\n",
    "styles = requests.get(\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/cs109.css\").text\n",
    "HTML(styles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Table of Contents\n",
    "\n",
    "<ol start=\"0\">\n",
    "  <li> Learning Goals </li>\n",
    "  <li> Principal Components Analysis </li>\n",
    "  <li> Logistic Regression </li>\n",
    "  <li> K-NN Classification </li>\n",
    "  <li> Model Evaluation Metrics</li>\n",
    "</ol>\n",
    "\n",
    "##### Learning Goals\n",
    "In this lab, we will look at how to use PCA to reduce a dataset to a smaller number of dimensions. The goal is for students to:\n",
    "<ul>\n",
    "  <li>Understand what PCA is and why it's useful</li>\n",
    "  <li>Feel comfortable performing PCA on a new dataset</li>\n",
    "  <li>Be able to extract the `variance explained` by components.</li>\n",
    "  <li>Perform modeling with the PCA components</li>\n",
    "</ul> \n",
    "\n",
    "Furthermore, we will look at the two forms of Classification:\n",
    "\n",
    "<ul>\n",
    "  <li>Logistic Regression</li>\n",
    "  <li>KNN Classsification</li>\n",
    "</ul>\n",
    "\n",
    "By the end of this lab, you should:\n",
    "- Be familiar with the `sklearn` implementations of\n",
    " - Logistic Regression\n",
    " - KNN Classification\n",
    "- Be able to evaluate the models based on different metrics (Confusion matrix and ROC curves)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "pd.set_option('display.width', 100)\n",
    "pd.set_option('display.max_columns', 20)\n",
    "plt.rcParams[\"figure.figsize\"] = (12,8)\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Part 1a:  PCA Review\n",
    "\n",
    "What is PCA? PCA is a deterministic technique to transform data to a lowered dimensionality, whereby each feature/dimension captures the most variance possible.\n",
    "\n",
    "Why do we care to use it?\n",
    "<ul>\n",
    "  <li>Visualizating the components can be useful</li>\n",
    "  <li>Allows for more efficient use of resources (time, memory)</li>\n",
    "  <li>Statistical reasons: fewer dimensions -> better generalization</li>\n",
    "  <li>noise removal / collinearity (improving data quality)</li>\n",
    "</ul>  \n",
    "\n",
    "Imagine some dataset where we have two features that are pretty redundant. For example, maybe we have data concerning elite runners. Two of the features may include ``VO2 max`` and ``heartrate``. These are highly correlated. We probably don't need both, as they don't offer much additional information from each other. Using a [great visual example from online](https://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues), let's say that this unlabelled graph **(always label your axis)** represents those two features:\n",
    "\n",
    "![VO2 Max vs Heart Rate](original_data.png)\n",
    "\n",
    "Let's say that this is our entire dataset, just 2 dimensions. If we wish to reduce the dimensions, we can only reduce it to just 1 dimension. A straight line is just 1 dimension (to help clarify this: imagine your straight line as being the x-axis, and values can be somewhere on this axis, but that's it. There is no y-axis dimension for a straight line). So, how should PCA select a straight line through this data?\n",
    "\n",
    "Below, the image shows all possible projects that are centered in the data:\n",
    "\n",
    "![Animation of possible lines](animation.gif)\n",
    "\n",
    "PCA picks the line that:\n",
    "<ul>\n",
    "<li>captures the most variance possible</li>\n",
    "<li>minimizes the distance of the transformed points (distance from the original to the new space)</li>\n",
    "</ul>\n",
    "\n",
    "The animation **suggests** that these two aspects are actually the same. In fact, this is objectively true, but the proof for which is beyond the scope of the material for now. Feel free to read more at [this explanation](https://stats.stackexchange.com/questions/32174/pca-objective-function-what-is-the-connection-between-maximizing-variance-and-m/136072#136072) and via [Andrew Ng's notes](http://cs229.stanford.edu/notes/cs229-notes-all/cs229-notes10.pdf).\n",
    "\n",
    "In short, PCA is a math technique that works with the covariance matrix -- the matrix that describes how all pairwise features are correlated with one another. Covariance of two variables measures the degree to which they moved/vary in the same direction; how much one variable affects the other. A positive covariance means they are positively related (i.e., x1 increases as x2 does); negative means negative correlation (x1 decreases as x2 increases).\n",
    "\n",
    "In data science and machine learning, our models are often just finding patterns in the data this is easier if the data is spread out across each dimension and for the data features to be independent from one another (imagine if there's no variance at all. We couldn't do anything). Can we transform the data into a new set that is a linear combination of those original features?\n",
    "\n",
    "PCA finds new dimensions (set of basis vectors) such that all the dimensions are orthogonal and hence linearly independent, and ranked according to the variance (eigenvalue). That is, the first component is the most important, as it captures the most variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Part 1b:  Intro to the Wine Dataset\n",
    "\n",
    "Imagine that a wine sommelier has tasted and rated 1,000 distinct wines, and now that she's highly experienced, she is curious if she can more efficiently rate wines without even trying them. That is, perhaps her tasting preferences follow a pattern, allowing her to predict the rating a new wine without even trying it!  \n",
    "\n",
    "The dataset contains 11 chemical features, along with a quality scale from 1-10; however, only values of 3-9 are actually used in the data. The ever-elusive perfect wine has yet to be tasted. \n",
    "\n",
    "#### **NOTE:** While this dataset involves the topic of alcohol, we, the s-CS109A staff, along with Harvard at large is in no way encouraging alcohol use, and this example should not be intrepreted as any endorsement for such; it is merely a pedagogical example. I apologize if this example offends anyone or is off-putting.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read-in and checking\n",
    "First, let's read-in the data and verify it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "wines_df = pd.read_csv(\"./data/wines.csv\", index_col=0)\n",
    "wines_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "wines_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this exercise, let's say that the wine expert is curious if she can predict, as a rough approximation, the **categorical quality -- bad, average, or great.** Let's define those categories as follows:\n",
    "\n",
    "- `bad` is when for wines that have a quality <= 5\n",
    "- `average` is when a wine has a quality of 6 or 7\n",
    "- `great` is when a wine has a quality of >= 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# copy the original data so that we're free to make changes\n",
    "wines_df_recode = wines_df.copy()\n",
    "\n",
    "# use the 'cut' function to reduce a variable down to the aforementioned bins (inclusive boundaries)\n",
    "wines_df_recode['quality'] = pd.cut(wines_df_recode['quality'],[0,5,7,10], labels=[0,1,2])\n",
    "wines_df_recode.loc[wines_df_recode['quality'] == 1]\n",
    "\n",
    "# drop the un-needed columns\n",
    "x_data = wines_df_recode.drop(['quality'], axis=1)\n",
    "y_data = wines_df_recode['quality']\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=.2, random_state=8, stratify=y_data)\n",
    "\n",
    "# previews our data to check if we correctly constructed the labels (we did)\n",
    "print(wines_df['quality'].head())\n",
    "print(wines_df_recode['quality'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For sanity, let's see how many wines are in each category:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data.value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now that we've split the data, let's look to see if there are any obvious patterns (correlations between different variables)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from pandas.plotting import scatter_matrix\n",
    "scatter_matrix(wines_df_recode, figsize=(30,20));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Part 1c: Dimensionality Reduction\n",
    "In attempt to improve performance when we model our data, we may wonder if some of our features are redundant and are posing difficulties for our logistic regression model. Let's PCA to shrink the problem down to 2 dimensions (with as little loss as possible) and see if that gives us a clue about what makes this problem tough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "num_components = 2\n",
    "\n",
    "\n",
    "# scale the datasets\n",
    "scale_transformer = StandardScaler(copy=True).fit(x_train)\n",
    "x_train_scaled = scale_transformer.transform(x_train)\n",
    "x_test_scaled = scale_transformer.transform(x_test)\n",
    "\n",
    "# reduce dimensions\n",
    "pca_transformer = PCA(num_components).fit(x_train_scaled)\n",
    "x_train_2d = pca_transformer.transform(x_train_scaled)\n",
    "x_test_2d =  pca_transformer.transform(x_test_scaled)\n",
    "\n",
    "print(x_train_2d.shape)\n",
    "x_train_2d[0:5,:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**NOTE:**\n",
    "1. Both scaling and reducing dimension follow the same pattern: we fit the object to the training data, then use .transform() to convert the training and test data. This ensures that, for instance, we scale the test data using the _training_ mean and variance, not its own mean and variance\n",
    "2. We need to equalize the variance of each feature before applying PCA; otherwise, certain dimensions will dominate the scaling. Our PCA dimensions would just be the features with the largest spread."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"exercise\"><b>Exercise 1.1:</b> Why didn't we scale the y-values (class labels) or transform them with PCA? Is this a mistake?</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "*Your Answer*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"exercise\"><b>Exercise 1.2:</b> Our data only has 2 dimensions/features now. What do these features represent?</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "*Your Answer*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our data only has 2 dimensions now, we can easily visualize the entire dataset. If we choose to color each datum with respect to its associated label/class, it allows us to see how separable the data is. That is, it gives an indication as to how easy/difficult it is for a model to fit the new, transformed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# notice that we set up lists to track each group's plotting color and label\n",
    "colors = ['r','c','b']\n",
    "label_text = [\"Bad Wines\", \"Average Wines\", \"Great Wines\"]\n",
    "\n",
    "# and we loop over the different groups\n",
    "for cur_quality in [0,1,2]:\n",
    "    cur_df = x_train_2d[y_train==cur_quality]\n",
    "    plt.scatter(cur_df[:,0], cur_df[:,1], c = colors[cur_quality], label=label_text[cur_quality])\n",
    "\n",
    "# all plots need labels\n",
    "plt.xlabel(\"PCA Dimension 1\")\n",
    "plt.ylabel(\"PCA Dimention 2\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Well, that gives us some idea of why the problem is difficult! The bad, average, and great wines are all on top of one another. Not only are there few great wines, which we knew from the beginning, but there is no line that can easily separate the classes of wines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"exercise\"><b>Exercise 1.3:</b></div>\n",
    "\n",
    "<ol>\n",
    "  <li>What critique can you make against the plot above? Why does this plot not prove that the different wines are hopelessly similar?</li>\n",
    "  <li>The wine data we've used so far consist entirely of continuous predictors. Would PCA work with categorical data?</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "*Your answer*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Looking at our PCA plot above, we see something peculiar: we have two disjoint clusters, both of which have immense overlap in the qualities of wines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"exercise\"><b>Exercise1.4:</b> What could cause this? What does this mean?</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "*Your answer*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"exercise\"><b>Exercise1.5:</b> Let's plot the same PCA'd data, but let's color code them according to if the wine is red or white. Does this graph help you answer our previous question? Does it change your thoughts?</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot each group\n",
    "\n",
    "# notice that we set up lists to track each group's plotting color and label\n",
    "colors = ['r','c','b']\n",
    "label_text = [\"Reds\", \"Whites\"]\n",
    "\n",
    "# and we loop over the different groups\n",
    "for cur_color in [0,1]:\n",
    "    cur_df = x_train_2d[x_train['red']==cur_color]\n",
    "    plt.scatter(cur_df[:,0], cur_df[:,1], c = colors[cur_color], label=label_text[cur_color])\n",
    "    \n",
    "# all plots need labels\n",
    "plt.xlabel(\"PCA Dimension 1\")\n",
    "plt.ylabel(\"PCA Dimention 2\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "*your answer*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Part 1d. Evaluating PCA: Variance Explained And Predictions\n",
    "One of the criticisms we made of the PCA plot was that it's lost something from the original data. Heck, we're only using 2 dimensions, we it's perfectly reasonable and expected for us to lose some information -- our goal was that the information we were discarding was noise.\n",
    "\n",
    "Let's investigate how much of the original data's structure the 2-D PCA captures. We'll look at the `explained_variance_ratio_` portion of the PCA fit. This lists, in order, the percentage of the x data's total variance that is captured by the nth PCA dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "var_explained = pca_transformer.explained_variance_ratio_\n",
    "print(\"Variance explained by each PCA component:\", var_explained)\n",
    "print(\"Total Variance Explained:\", np.sum(var_explained))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The first PCA dimension captures 33% of the variance in the data, and the second PCA dimension adds another 20%. Together, we've captured about half of the total variation in the training data with just these two dimensions. So far, we've used PCA to transform our data, we've visualized our newly transformed data, and we've looked at the variance that it captures from the original dataset. That's a good amount of inspection; now let's actually use our transformed data to make predictions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're only using 2 dimensions. What if we increase our data to 10 PCA components?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"exercise\"><b>Exercise 1.6:</b></div>\n",
    "<ol>\n",
    "  <li>Fit a PCA that finds the first 10 PCA components of our training data</li>\n",
    "  <li>Use `np.cumsum()` to print out the variance we'd be able to explain by using n PCA dimensions for n=1 through 10</li>\n",
    "  <li>Does the 10-dimension PCA agree with the 2d PCA on how much variance the first components explain? **Do the 10d and 2d PCAs find the same first two dimensions? Why or why not?**</li>\n",
    "    <li>Make a plot of number of PCA dimensions against total variance explained. What PCA dimension looks good to you?</li>\n",
    "</ol> \n",
    "\n",
    "Hint: `np.cumsum` stands for 'cumulative sum', so `np.cumsum([1,3,2,-1,2])` is `[1,4,6,5,7]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(***,***)\n",
    "plt.xlabel(\"PCA Dimension\")\n",
    "plt.ylabel(\"Total Variance Captured\")\n",
    "plt.title(\"Variance Explained by PCA\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The plot above can be used to inform of us when we reach diminishing returns on variance explained. That is, the 'elbow' of the line is probably an ideal number of dimensions to use, at least with respect to the amount of variance explained.\n",
    "\n",
    "<br>\n",
    "\n",
    "<div class=\"exercise\"><b>Exercise 1.7:</b> Looking at your graph, what is the 'elbow' point / how many PCA components do you think we should use? Does this number of components imply that predictive performance will be optimal at this point? Why or why not?</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "*Your answer*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1e: PCA Debriefing:\n",
    "\n",
    "- PCA maps a high-dimensional space into a lower dimensional space.\n",
    "- The PCA dimensions are ordered by how much of the original data's variance they capture\n",
    "    - There are other cool and useful properties of the PCA dimensions (orthogonal, etc.). See a [textbook](http://math.mit.edu/~gs/linearalgebra/).\n",
    "- PCA on a given dataset always gives the same dimensions in the same order.\n",
    "- You can select the number of dimensions by fitting a big PCA and examining a plot of the cumulative variance explained.\n",
    "\n",
    "PCA is not guaranteed to improve predictive performance at all. As you've learned in class now, none of our models are guaranteed to always outperform others on all datasets; analyses are a roll of the dice. The goal is to have a suite of tools to allow us to gather, process, disect, model, and visualize the data -- and to learn which tools are better suited to which conditions. Sometimes our data isn't the most conducive to certain tools, and that's okay.\n",
    "\n",
    "What can we do about it?\n",
    "1. Be honest about the methods and the null result. Lots of analyses fail.\n",
    "2. Collect a dataset you think has a better chance of success. Maybe we collected the wrong chemical signals...\n",
    "3. Keep trying new approaches. Just beware of overfitting the data you're validating on. Always have a test set locked away for when the final model is built.\n",
    "4. Change the question. Maybe something you noticed during analysis seems interesting or useful (classifying red versus white). But again, you the more you try, the more you might overfit, so have test data locked away.\n",
    "5. Just move on. If the odds of success start to seem small, maybe you need a new project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Part 2: Logistic Regression \n",
    "\n",
    "The logistic regression model is used to model the probability of a data to belong to a certain class/category. For eg:\n",
    "\n",
    "![Cow](cow1.jpg) ![Exam_pass_curve](exam_curve.jpeg) Ref: https://en.wikipedia.org/wiki/Logistic_regression\n",
    "\n",
    "Mathematically, this can be written as:\n",
    "$$l = \\log_e\\left(\\frac{p}{1-p}\\right) = \\beta_0 +\\beta_1 X_1+\\beta_2 X_2 $$\n",
    "\n",
    "The equation can be used to obtain the probability for the give data to exist in a particlar class. They can be assigned particular classes bases on this probability using a cut-off condition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance of a simple classification model\n",
    "\n",
    "Let us first look at the performance of a simplest classification model (MLE) on the data. The model under consideration is not the best one to use on such data, and we will be able to see why.\n",
    "\n",
    "We will explore the Wines dataset which has 3 classes \"Bad Wines\", \"Average Wines\" and \"Great Wines.\" This is a mult-class problem where we have 3 categories that have been defined as 0, 1 and 2 in our dataframes previously. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Maximum-likelihood estimation (MLE)\n",
    "\n",
    "MLE is a naive model that is doesnt require any training or any parameters (ignoring your preictors). It selects class value that makes the observed data most probable, thereby maximizing the likelihood function. Essentially, it returns the class that is seen in the data with the most freuency. So in our case, if you were to pick a wine at random, it would give you the class that it could most probably belong to. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mle_y = y_data.value_counts().idxmax()\n",
    "print('The MLE class is:', mle_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class 1 corresponds to wine being classified as average. This makes sense as naively saying a wine you pick at random could be of average quality is a sensible argument. Let see how true it actually is in the next excercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exercise\"><b>Exercise 2.1:</b>  Use the train data, find the MLE and y that is the most probable, where 𝑦∈{0,1,2}. Check what is the accuracy of the model on the train and test data (the % that are correctely classified).</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Answer\n",
    "mle_y = ***\n",
    "\n",
    "train_accuracy = ***\n",
    "\n",
    "test_accuracy = ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The train MLE class is:', mle_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it looks like just predicting the class to be MLE without looking at the x data is not a great approach, we seem to be doing a little better than selecting a class at random. Let's see how our classsification models fare!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single predictor Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See [.predict()](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) documentation here. **NOTE:** regularization is applied by default. Especially pay attention to the following arguments/parameters:\n",
    "\n",
    "- `C` penalty, which we discussed in class. Experiment with varying values from 0 to 100 million! \n",
    "- `max_iterations`: experiment with values from 5 to 5000. Do you expect more iterations to always perform better? Why or why not?\n",
    "- `penalty`: for designating 'l1' (Lasso) or 'l2' (Ridge) loss; default is 'l2' (can be set to 'none' to get unregularized estimates)\n",
    "- `solver`: especially for the multi-class setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logref = LogisticRegression(fit_intercept=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train1 = x_train['red'].to_numpy().reshape(-1,1)\n",
    "x_test1 = x_test['red'].to_numpy().reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logref.fit(x_train1, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_test = logref.predict(x_test1)\n",
    "initial_score = accuracy_score(y_test.to_numpy(), y_hat_test)\n",
    "b0 = logref.intercept_\n",
    "b1 = logref.coef_\n",
    "print(\"Our single predictor logistic regression model yields accuracy score of:\", initial_score)\n",
    "print(\"The estimated coefficients for this simple logistic regression model are: \\n\", b0, \"\\n\", b1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_accuracy = -1\n",
    "best_model = None\n",
    "\n",
    "c_vals = [0.001, 0.1, 1, 10, 100]\n",
    "for c_val in c_vals:\n",
    "    \n",
    "    logref = LogisticRegression(C=c_val, solver='liblinear', max_iter=1000)\n",
    "    logref.fit(x_train1, y_train)\n",
    "    y_hat_test = logref.predict(x_test1)\n",
    "    cur_accuracy = accuracy_score(y_test.to_numpy(), y_hat_test)\n",
    "\n",
    "    if cur_accuracy > best_accuracy:\n",
    "        best_accuracy = cur_accuracy\n",
    "        best_model = logref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best found logistic regression model:\", best_model,\"\\nAccuracy score:\", best_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logistic regression model already outperforms the naive MLE model.\n",
    "\n",
    "**NOTE:** \n",
    "\n",
    "- The reason we do not use linear regression here is because our goal is different here. This is a **classification** problem ( where we are concerned with predicting outputs that belong to certain categories), and not **regression** problem (predicting outputs which can be continuous-valued and can take any value).\n",
    "\n",
    "- Another reason that Linear Regression does not work here is that the ordering of classes (0,1,2) is arbitrarily chosen by us. There is no reson to assign them as such, we could swap them around and the structure of the problem and the output remains the same. This would not work for Linear Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi predictor Logistic Regression\n",
    "\n",
    "Now lets use all of our predictors and biuld our model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exercise\"><b>Exercise 2.2:</b> Use the full training data to fit the Multiple predictor Logistic Regression model. Vary the fit paramters (C value, number of iterations, solver, etc) and see if you can increase the accuracy. What do you notice? </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After fitting the model, you can print the ``.coef_`` value to see its $\\hat{\\beta}$ coefficient estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your answer\n",
    "best_accuracy = -1\n",
    "best_model = None\n",
    "\n",
    "# experiment with different values\n",
    "c_vals = [100000]\n",
    "num_iters = [100]\n",
    "for c_val in c_vals:\n",
    "    for num_iter in num_iters:\n",
    "        logref = LogisticRegression(***)\n",
    "        logref.fit(***, ***)\n",
    "        y_hat_test = ***\n",
    "        cur_accuracy = ***\n",
    "\n",
    "        if cur_accuracy > best_accuracy:\n",
    "            best_accuracy = cur_accuracy\n",
    "            best_model = logref\n",
    "            best_iter = num_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best found logistic regression model:\", best_model, \"\\nNo of iterations needed :\",best_iter,\"\\nAccuracy score:\", best_accuracy, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of Learned coefficients:\", len(best_model.coef_[0]))\n",
    "print(\"The coefficients should match the number of features we have:\", x_test.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(x_test.columns)):\n",
    "    print(\"Feature:\", x_test.columns[i], \";  Coef:\", best_model.coef_[0][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** If the different predictors have different range of values, it is generally a good idea to standardize or normalize them...especially when using regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization in Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_regr = LogisticRegressionCV(Cs=10, solver='liblinear', penalty='l2', cv=10)\n",
    "ridge_regr.fit(x_train, y_train)\n",
    "\n",
    "# predictions\n",
    "y_train_pred = ridge_regr.predict(x_train) \n",
    "y_test_pred = ridge_regr.predict(x_test)\n",
    "\n",
    "# accuracy\n",
    "train_score = accuracy_score(y_train, y_train_pred)\n",
    "test_score = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "print('Train accuracy after regression = ', train_score)\n",
    "print('Test accuracy after regression = ', test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exercise\"><b>Exercise 2.3:</b> Perform 5-fold cross validation to obtain the best Ridge Regularised Logistic Regression model. </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** Each of the values in Cs describes the inverse of regularization strength. If Cs is as an int, then a grid of Cs values are chosen in a logarithmic scale between 1e-4 and 1e4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your answer\n",
    "best_accuracy = -1\n",
    "best_model = None\n",
    "\n",
    "# experiment with different values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train accuracy after regression = ', train_score)\n",
    "print('Test accuracy after regression = ', test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We were able to improve the accuracy ever so slightly! :')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exercise\"><b>Exercise 2.4:</b> Use Logistic Regression (with and without cross-validation) on the PCA-transformed data.Logistic regression was performed on original data and accuracy of approximately 75% was achieved. Before you generate an answer, do you expect to outperform the 75% with PCA transformed data. What are your actual results? Does this seem reasonable? .</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your answer\n",
    "# partial solution provided (not showing CV portion)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr_pca = LogisticRegression(C=1000000, solver='lbfgs', multi_class='ovr', max_iter=10000).fit(x_train_2d,y_train)\n",
    "\n",
    "df_results = pd.DataFrame(scores, index=names, columns=['Train Accuracy', 'Test Accuracy'])\n",
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reference: https://scipython.com/blog/plotting-the-decision-boundary-of-a-logistic-regression-model/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression is a parametric model ideally used to estimate associations between a response ($Y$) and predictors ($X$s), and does that by estimating the probability of *success* based on a Bernoulli distribution:\n",
    "$$Y_i|X_1,...,X_p \\sim \\text{Bern}\\left(p=\\frac{1}{1+e^{-(\\beta_0+\\beta_1X_1+...+\\beta_pX_p)}}\\right)$$\n",
    "\n",
    "The estimate of the probability, $\\hat{p}_i$,  that $Y_i=1$ can be turned into a pure classification: naturally based on whether $\\hat{p}_i > 0.5$.\n",
    "\n",
    "**Note:** in the multiclass setting, there is no guarantee that a single class's probability estimate will be above the boundary 0.5, and so the class with the maximum estimated probability is chosen instead "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code adopted from: https://scikit-learn.org/stable/auto_examples/linear_model/plot_iris_logistic.html\n",
    "\n",
    "label_text = [\"Bad\", \"Average\", \"Great\"]\n",
    "\n",
    "dim1 = \"PCA 1\"\n",
    "dim2 = \"PCA 2\"\n",
    "\n",
    "# Plotting decision regions\n",
    "X = x_train_2d\n",
    "Y = y_train\n",
    "# Create an instance of Logistic Regression Classifier and fit the data.\n",
    "logregplot = LogisticRegression(C=1000, solver='liblinear')\n",
    "logregplot.fit(X, Y)\n",
    "\n",
    "\n",
    "# Plot the decision boundary. \n",
    "x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "h = .02  # step size in the mesh\n",
    "\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "Z = logregplot.predict(np.stack((xx.ravel(), yy.ravel()), axis=1))\n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.figure(1, figsize=(8, 6))\n",
    "plt.pcolormesh(xx, yy, Z, shading='auto', cmap=plt.get_cmap('Paired',3))\n",
    "\n",
    "\n",
    "# Plot also the training points\n",
    "sc = plt.scatter(X[:, 0], X[:, 1],  c = Y, edgecolors='k', cmap=plt.get_cmap('Paired',3))\n",
    "plt.xlabel(dim1)\n",
    "plt.ylabel(dim2)\n",
    "plt.title(\"Decision Boundary for classification\")\n",
    "\n",
    "# produce a legend from the scatter\n",
    "plt.legend(handles=sc.legend_elements()[0], labels = label_text)\n",
    "\n",
    "plt.xlim(xx.min(), xx.max())\n",
    "plt.ylim(yy.min(), yy.max())\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.unique(Z,return_counts=True))\n",
    "np.unique(Y.values,return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a Multiclass Logistic Regression with more than 2 predictors, the decision boundary may be hard to visualize geometrically, so the first 2 PCA vectors are used to be able to plot it on the screen.  Even though the prediction accuracy may be high, the resulting visualization may not show it.  Why? Baacuse what might seem inseparable could be separable when the data is projected onto the next PCA axes (eg. PCA3 and PCA4 dimensions), but is useful in centrain situations to visualize the decision boundary. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exercise\"><b>Exercise 2.5:</b> Comment on the decision boundary above for the multiple logistic regressiopn model.  Does it seem to accurately reflect the classification model?  </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer:*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exercise\"><b>Exercise 2.6:</b> There are clearly 2 classes of wines that are easily separable (though quality is difficult to predict).  What do you suppose those classes may represent?  What could this suggest for possible decisions in feature engineering (hint: think interactions)? </div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer:*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: KNN Classificaton\n",
    "\n",
    "Ref: https://medium.com/analytics-vidhya/knn-k-nearest-neighbors-1add1b5d6eb2\n",
    "\n",
    "![knn](knn.png) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model assigns the data to different categories based on the k nearest neighbours from it. The distance is usually the Eucledean distance in feature space, but could also be based on other metrics like Minkowski distance, Manhattan distance, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(10)\n",
    "knn.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Classification accuracy for knn10: \\n Train =\",\n",
    "    knn.score(x_train,y_train),\", Test =\", knn.score(x_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exercise\"><b>Exercise 3.1: Find the Classification accuracy by fitting a a few different KNN models with k of oyur choice. How does the accuracy vary? What happens when k = 1?</b> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your answer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might see that 1NN has a very high train accuracy. The model assigns a data point to the class of its closest neighbour in the feature space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exercise\"><b>Exercise 3.2: Plot the variation of classification accuracy (in train) with different k, k={1,3,5,7,10,15,25,100}</b> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your answer\n",
    "from sklearn.model_selection import cross_val_score #Another neat way to CV\n",
    "\n",
    "acc = []\n",
    "k_vals = ***\n",
    "\n",
    "for k in k_vals:\n",
    "    knn_model = ***\n",
    "    acc.append(***) \n",
    "plt.figure(figsize = (8,6))    \n",
    "plt.plot(k_vals, acc)\n",
    "plt.xlabel('k value')\n",
    "plt.ylabel('Classification accuracy')\n",
    "plt.title(\"Classification accuracy vs k\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it seems like k = 20 has the highest accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exercise\"><b>Exercise 3.3: Plot the classification boundary of the best performing model. Use the PCA1 and PCA2 as the axes</b> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your answer\n",
    "\n",
    "n = 100\n",
    "\n",
    "x1=np.linspace(np.min(x_train_2d[:,0]),np.max(x_train_2d[:,0]),n)\n",
    "x2=np.linspace(np.min(x_train_2d[:,1]),np.max(x_train_2d[:,1]),n)\n",
    "x1v, x2v = np.meshgrid(x1, x2)\n",
    "\n",
    "knn = KNeighborsClassifier(***)\n",
    "knn.fit(***,***)\n",
    "\n",
    "# To do the predictions and keep the yhats on 2-D (to match the dummy predictor shapes), use this\n",
    "yhat = knn.predict(np.c_[x1v.ravel(), x2v.ravel()])\n",
    "plt.figure(figsize = (8,8))\n",
    "plt.pcolormesh(x1v, x2v, yhat.reshape(x1v.shape)) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Part 4: Evaluate Models via Confusion matrices and ROC Curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the methods of evaluating Classification models are Confusion matrices and ROC curves.\n",
    "\n",
    "### Confusion Matrix\n",
    "As a reminder of Confustion Matrices:\n",
    "- the samples that are +ive and the classifier predicts as +ive are called True Positives (TP)\n",
    "- the samples that are -ive and the classifier predicts (wrongly) as +ive are called False Positives (FP)\n",
    "- the samples that are -ive and the classifier predicts as -ive are called True Negatives (TN)\n",
    "- the samples that are +ive and the classifier predicts as -ive are called False Negatives (FN)\n",
    "\n",
    "IMPORTANT NOTE: In sklearn, to obtain the confusion matrix in the standard form, always have the observed `y` first, i.e.: use as `confusion_matrix(y_true, y_pred)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the third class. ROC curves work well with binary classification\n",
    "x_train_new = x_train[y_train!=2]\n",
    "y_train_new = y_train[y_train!=2]\n",
    "\n",
    "x_test_new = x_test[y_test!=2]\n",
    "y_test_new = y_test[y_test!=2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "knn20 = KNeighborsClassifier(20)\n",
    "knn20.fit(x_train_new,y_train_new)\n",
    "\n",
    "y_pred_train = knn20.predict(x_train_new)\n",
    "y_pred_test = knn20.predict(x_test_new)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(C=1000, solver='liblinear')\n",
    "logreg.fit(x_train_new,y_train_new)\n",
    "\n",
    "y_log_pred_train = logreg.predict(x_train_new)\n",
    "y_log_pred_test = logreg.predict(x_test_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exercise\"><b>Exercise 4.1: Obtain the confusion matrix for the Classsification models above. Which of the two models do you think performs better? </b> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your answer\n",
    "conf_mat_knn = pd.DataFrame(\n",
    "    confusion_matrix(y_test_new, y_pred_test), \n",
    "    index=['true:0', 'true:1'], \n",
    "    columns=['pred:0', 'pred:1']\n",
    ")\n",
    "\n",
    "conf_mat_knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer:*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpreting the Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confusion matrix takes a classification model's evluation a bit deeper than simply using overall classification accuracy, but it has its limitations.  The two major ones are:\n",
    "\n",
    "1. If your classes are severely unbalanced, then your model(s) may predict everyone to be in a single class when using the Bayes' classifier (think: what if 99% of your data are all in class 1?)\n",
    "2. There still could be a lot of ties when comparing models.  For example, changing the regularization parameter from 0.01 to 0.1 may in the end result in the same rates of false positives and false negatives.\n",
    "\n",
    "A more holistic approach (improving the two issues mentioned above) to evaluating classificiation models is preferred."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ROC curve (receiver operating characteristic curve) improves on the use of a single confusion matrix.  It essentially created a large number of confusion matrices at prespecified values of the decision threshold (not just 0.5), and plots the resulting true positive and false positive rates.  It gives a broader picture to aid in evaluating a single or several different classification models.\n",
    "\n",
    "Note: it works only for a binary outcome!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####\n",
    "#Make ROC curves to evaluate a model's overall useability.\n",
    "#####\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import seaborn as sns\n",
    "\n",
    "# a function to make 'pretty' ROC curves for this model\n",
    "def make_roc(name, clf, ytest, xtest, ax=None, labe=5, proba=True, skip=0):\n",
    "    initial=False\n",
    "    if not ax:\n",
    "        ax=plt.gca()\n",
    "        initial=True\n",
    "    if proba:#for stuff like logistic regression\n",
    "        fpr, tpr, thresholds=roc_curve(ytest, clf.predict_proba(xtest)[:,1])\n",
    "        \n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    if skip:\n",
    "        l=fpr.shape[0]\n",
    "        ax.plot(fpr[0:l:skip], tpr[0:l:skip], '.-', alpha=0.3, label='ROC curve for %s (area = %0.2f)' % (name, roc_auc))\n",
    "    else:\n",
    "        ax.plot(fpr, tpr, '.-', alpha=0.3, label='ROC curve for %s (area = %0.2f)' % (name, roc_auc))\n",
    "    label_kwargs = {}\n",
    "    label_kwargs['bbox'] = dict(\n",
    "        boxstyle='round,pad=0.3', alpha=0.2,\n",
    "    )\n",
    "    if labe!=None:\n",
    "        for k in range(0, fpr.shape[0],labe):\n",
    "            #from https://gist.github.com/podshumok/c1d1c9394335d86255b8\n",
    "            threshold = str(np.round(thresholds[k], 2))\n",
    "            ax.annotate(threshold, (fpr[k], tpr[k]), **label_kwargs)\n",
    "    if initial:\n",
    "        ax.plot([0, 1], [0, 1], 'k--')\n",
    "        ax.set_xlim([0.0, 1.0])\n",
    "        ax.set_ylim([0.0, 1.05])\n",
    "        ax.set_xlabel('False Positive Rate')\n",
    "        ax.set_ylabel('True Positive Rate')\n",
    "        ax.set_title('ROC')\n",
    "    ax.legend(loc=\"lower right\")\n",
    "    return ax\n",
    "\n",
    "\n",
    "sns.set_context(\"poster\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_roc(\"Logistic\", logreg, y_test_new, x_test_new, ax=None, labe=20, proba=True, skip=1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exercise\"><b>Exercise 4.2: Obtain a ROC curve for the knn20 model </b> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exercise\"><b>Exercise 4.3: What do the number superimposed on the ROC curve represent?  Which of the two models compared above is better based on the ROC curves?  Based on the AUC?  Is that model always superior given a False Positive Rate?  What would AUC value of 0.5 mean?  </b> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer:*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus: Using statsmodels to perform inference in logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of sklearn, we can also use `statsmodels` for classification. It also allows use to perform some inferences while sing the logistic regression. An example is shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "\n",
    "x_train_const = sm.add_constant(x_train_new)\n",
    "x_test_const = sm.add_constant(x_test_new)\n",
    "\n",
    "\n",
    "sm_model = sm.Logit(y_train_new, x_train_const).fit()\n",
    "\n",
    "print(sm_model.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exercise\"><b>Exercise 4.4: Interpret the output above.  Which variable is most strongly related to the response in the cointext of this model?  Interpret the results for the variable $\\texttt{density}$.  </b> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer:*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_sm_pred_train = sm_model.predict(x_train_const)>.5\n",
    "y_sm_pred_test = sm_model.predict(x_test_const)>.5\n",
    "\n",
    "#calculate the accuracy\n",
    "print(f\"Train Accuracy: {(y_train_new==y_sm_pred_train).mean()} \\nTest Accuracy: {(y_test_new==y_sm_pred_test).mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
