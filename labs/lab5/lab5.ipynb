{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "double-stupid",
   "metadata": {},
   "source": [
    "# <img style=\"float: left; padding-right: 10px; width: 45px\" src=\"img/summer_school.png\"> S-109A Introduction to Data Science \n",
    "\n",
    "\n",
    "## Lab 5: Ensembling methods, and Neural Networks\n",
    "\n",
    "**Harvard University**<br>\n",
    "**Summer 2021**<br>\n",
    "**Instructor:** Kevin Rader<br>\n",
    "**Notebook Authors:** Will Claybaugh, David Sondak, Kevin Rader, Chris Gumb\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "architectural-strike",
   "metadata": {},
   "source": [
    "## <font color='red'> Run the cell below to properly highlight the exercises</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "skilled-authentication",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "style = \"<style>\"+\\\n",
    "    \"div.exercise { background-color: #bf6cf1;border-color: #E9967A; border-left: 5px solid #800080; padding: 0.5em;}\"+\\\n",
    "    \"div.discussion { background-color: #90EE90;border-color: #E9967A; border-left: 5px solid #800080; padding: 0.5em;}\"+\\\n",
    "    \"</style>\"\n",
    "HTML(style)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tribal-utilization",
   "metadata": {},
   "source": [
    "## Learning Goals\n",
    "After this lab, you should be able to:\n",
    "- Discuss when combining several models could help improve overall accuracy\n",
    "- Tune across multiple hyperparameters using cross_val_scores, GridSearchCV, or ParamGrid\n",
    "- Explain why boosting and bagging are particularly effective versions of ensembling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entitled-benchmark",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "pd.set_option('display.width', 500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.notebook_repr_html', True)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from IPython import display\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accepting-georgia",
   "metadata": {},
   "source": [
    "## Part 0: Jellybeans?\n",
    "<img src='img/jelly_beans.jpg' style='width:200px;'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tired-surveillance",
   "metadata": {},
   "source": [
    "Imagine we'd brought a jar of jelly beans to lab today and asked everyone to guess how many are in the jar. Whoever guesses closest to the exact number of jelly beans gets to keep them all! (If several people are the same distance from the truth, they split the jar equally.)\n",
    "\n",
    "In this example, the members of the class play the role of a single model/estimator, and by combining the class's predictions we get a more accurate overall estimate. When we combine multiple models, it's called _ensembling_. \n",
    "\n",
    "We can learn a lot about ensembling by imagining a group of people all guessing how many beans are in a jar. What features would we want those people to have? What biases or pathologies could ruin the overall ensemble?\n",
    "\n",
    "<div class=\"discussion\"><b>Discussion</b></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cardiac-relationship",
   "metadata": {},
   "source": [
    "**Q0.1** What happens to the combined guess if each member of the class makes the same kind of mistakes?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medieval-cartoon",
   "metadata": {},
   "source": [
    "*your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "considerable-commitment",
   "metadata": {},
   "source": [
    "**Q0.2** What happens if the class is all clones and all make the same guess?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "common-palestinian",
   "metadata": {},
   "source": [
    "*your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prospective-ozone",
   "metadata": {},
   "source": [
    "**Q0.3** What happens if a few classmembers are really, really, really bad at guessing?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chronic-antigua",
   "metadata": {},
   "source": [
    "*your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clear-kelly",
   "metadata": {},
   "source": [
    "**Q0.4** What happens if people are influenced by the previous guess, or by peer pressure?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qualified-emphasis",
   "metadata": {},
   "source": [
    "*your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "physical-circumstances",
   "metadata": {},
   "source": [
    "**Q0.5** Are there better combination rules than 'take the average'?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "revised-engineering",
   "metadata": {},
   "source": [
    "*your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affected-offense",
   "metadata": {},
   "source": [
    "**Q0.6** What happens if there are multiple jars in which each classmate provides a separate guesss for?  Is there an issue if people who guess too high on one jar tend to guess too high on another?  What if guessing too high on one means you'll likely guess too low on the other?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greek-malpractice",
   "metadata": {},
   "source": [
    "*your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accompanied-montreal",
   "metadata": {},
   "source": [
    "**Q0.7** How should we deal with people who hate jelly beans, and purposefully guessed wrong about that jar?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complimentary-sellers",
   "metadata": {},
   "source": [
    "*your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "muslim-bronze",
   "metadata": {},
   "source": [
    "**Q0.8** Putting it all together, what would a good group of estimators look like? What would a good combination rule look like?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visible-plenty",
   "metadata": {},
   "source": [
    "*your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "configured-slovakia",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interior-stopping",
   "metadata": {},
   "source": [
    "## Part 1: Combining Models\n",
    "The data today are from https://www.kaggle.com/hobako1993/sp1-factor-binding-sites-on-chromosome1/home:\n",
    "\n",
    ">This dataset includes SP1 transcription factor binding and non-binding sites on human chromosome1. It can be used for binary classification tasks in bioinformatics. There are 1200 sequences for binding sites (BS) and 1200 sequences for non-binding sites (nBS). We have labeled sequences with 1 for BS and 0 for nBS. Each sequence is 14 nucleobase length, which is converted to numeric string using codes below, assigned to each nucleobase 00 for A 01 for T 10 for C 11 for G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "trained-parent",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('data/genes_train.csv', index_col=0)\n",
    "df_test = pd.read_csv('data/genes_test.csv', index_col=0)\n",
    "\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "established-forum",
   "metadata": {},
   "source": [
    "#### The tuning set\n",
    "In addition to the usual train/test split above, we further split our training data into \n",
    "1. A true training set, and \n",
    "2. A held-out tuning set which we'll use to learn rules that combine base models.\n",
    "\n",
    "|training set|tuning set|test set|\n",
    "|--|--|--|\n",
    "|Build lots of models, including tuning their hyperparameters by doing CV within these data| Learn how to combine base models so that the combination better predicts data the models haven't seen| Find out how good the models+combination is on data it hasn't seen\n",
    "\n",
    "The overall pipeline would look like this\n",
    "1. Fit models on the training data, finding hyperparameters via CV if needed\n",
    "2. Have each model make a prediciton on the tuning data\n",
    "3. Learn how to best combine the models' tuning predictions. For instance, fit a linear regression with the model predictions as the input and the true y values as the output\n",
    "4. Evaluate the models+combination rule on the test data:\n",
    "    1. Have each model predict each test point\n",
    "    2. Use the rule/model from 3 to combine the predictions into a single prediciton for each point\n",
    "    3. Compare the predictions above to the true y values\n",
    "5. In production, each model makes prediciton on the new data, and we use the rule/model from 3 to combine those into a final prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "flying-jason",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# further split the training data into training and tuning sets\n",
    "df_train, df_tune = train_test_split(df_train, test_size=.2, random_state=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "laughing-winning",
   "metadata": {},
   "source": [
    "#### Design matrices\n",
    "In each set, we build the design matrix and the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oriented-speech",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response_and_design(df):\n",
    "    design = df.iloc[:, df.columns != 'label']\n",
    "    response = df['label'].values\n",
    "    \n",
    "    return response, design\n",
    "\n",
    "y_train, X_train = get_response_and_design(df_train)\n",
    "y_tune, X_tune = get_response_and_design(df_tune)\n",
    "y_test, X_test = get_response_and_design(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "younger-seattle",
   "metadata": {},
   "source": [
    "## Ensembling\n",
    "In lecture, we've already discussed how boosting and bagging build several base models and then combine them. Ensembling is the general practice of combining individual base models in order to get a more accurate overall estimate. In a sense, ensembling builds a meta-model that combines predictions from original base models.\n",
    "\n",
    "**The two pieces of any ensemble are:**  \n",
    "**1. The original base models**  \n",
    "**2. The rule for combining the base models' predictions**\n",
    "** **"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "healthy-peace",
   "metadata": {},
   "source": [
    "<div class=\"discussion\"><b>Discussion</b></div>\n",
    "<br>\n",
    "\n",
    "**Q1.1** What are Bagging and Boosting's methods of building esnemble models?  What combination rules do they use?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tamil-civilian",
   "metadata": {},
   "source": [
    "**Answer**:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brilliant-globe",
   "metadata": {},
   "source": [
    "**Q1.2** What would Bagging's models look like as a room full of people? What would Boosting's models look like as a room full of people?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "artistic-combine",
   "metadata": {},
   "source": [
    "**Answer**:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "removed-chrome",
   "metadata": {},
   "source": [
    "### Building our ensemble\n",
    "Here, we'll take the original base models as given. Each base model in the array below has been trained on the training dataset.\n",
    "\n",
    "For those who haven't seen the `.joblib` format before, it's SKLearn's preferred method for saving modeles to disk. \n",
    "You can read more here: https://scikit-learn.org/stable/modules/model_persistence.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "committed-equation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You'll have to install `joblib` if you don't have it already\n",
    "!pip install joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aquatic-imperial",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# load all our saved models into a list\n",
    "models = []\n",
    "with open('data/models.joblib', 'rb') as f:\n",
    "    while True:\n",
    "        try:\n",
    "            models.append(joblib.load(f))\n",
    "        except EOFError:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "specific-branch",
   "metadata": {},
   "source": [
    "Individually, the models are rather poor, and don't even beat a logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "positive-cartridge",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "# get the accuracy score from logistic regression\n",
    "# cross validation to determine the best regularization parameter, C\n",
    "LR_score = LogisticRegressionCV().fit(X_train, y_train).score(X_test,y_test)\n",
    "\n",
    "scores = []\n",
    "for cur_model in models:\n",
    "    scores.append(cur_model.score(X_test,y_test))\n",
    "\n",
    "fig, ax = plt.subplots(1,1,figsize=(20,10))\n",
    "ax.hist(scores,20, label=\"Decision Tree Base models\");\n",
    "ax.axvline(LR_score, color='red',label=\"Logistic Regression\")\n",
    "ax.set_xlabel('Accuracy Scores', fontsize=24) # DLS:  Added x-label\n",
    "ax.set_ylabel(\"Number of Models at Each Score\")\n",
    "ax.legend(loc='best', fontsize=24)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rolled-consolidation",
   "metadata": {},
   "source": [
    "To summarize the models, we have data frames recording each model's prediction on each point in the training/tuning/test set.\n",
    "\n",
    "The 'tuning' set below is new. As mentioned, we need a set of data that isn't the training data, nor used to set hyperparameters in the original models (so, not the validation set), nor the final test data. We'll use this data in just a bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expanded-million",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_train = pd.read_csv(\"data/predictions_train.csv\", index_col=0)\n",
    "predictions_tune = pd.read_csv(\"data/predictions_tune.csv\", index_col=0)\n",
    "predictions_test = pd.read_csv(\"data/predictions_test.csv\", index_col=0)\n",
    "\n",
    "predictions_tune.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wicked-solution",
   "metadata": {},
   "source": [
    "Considering the first row above, we see that model0 predicted the first tuning point to be class 0, model1 disagreed and thought it was class 1, and most of the other models think it's class 0. Other rows tell us about other observations in the tuning set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "correct-palmer",
   "metadata": {},
   "source": [
    "### Combining the models\n",
    "Perhaps the simplest way of combining the models' predictions (above) is a majority vote. Let's compute the test score under this rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lyric-girlfriend",
   "metadata": {},
   "outputs": [],
   "source": [
    "# axix = 1: get the mean along a row\n",
    "meta_test_predictions = np.mean(predictions_test,axis=1) > 0.5  #do we have more than 50% 1s?\n",
    "print(\"Test accuracy (Classify by majority vote): \", accuracy_score(y_test, meta_test_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "printable-token",
   "metadata": {},
   "source": [
    "That's quite a bit better! The majority vote is scoring far better than any individual model.\n",
    "\n",
    "**Note**:\n",
    "- Working with probabilities is more information-rich. Especially in the two-class setting, predictions take probabilities like .51 and pretend that they're actually 1.0. You'll deal more with ensembling via probabilities on your homework. For now, we'll stick with the models' predictions.\n",
    "- The Receiver Operating Characteristic Area Under the Curve (ROC AUC) is generally considered a more reliable metric than accuracy for a classification problem. Feel free to also use <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html\" target=\"_blank\">roc_auc(y_test, y_proba)</a> to score the models in this notebook.\n",
    "\n",
    "** **"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hazardous-venice",
   "metadata": {},
   "source": [
    "<div class=\"discussion\"><b>Discussion</b></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "engaging-blank",
   "metadata": {},
   "source": [
    "**Q1.3** Suppose each modelling process reflected in the ensemble has the same bias and variance. How does majority voting affect combined bias and variance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broke-entrepreneur",
   "metadata": {},
   "source": [
    "**Answer**:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "everyday-clark",
   "metadata": {},
   "source": [
    "**Q1.4** Can you think of a weakness in majority voting? (Recall the original models that had less than 50% accuracy...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reflected-crime",
   "metadata": {},
   "source": [
    "**Answer**:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electoral-career",
   "metadata": {},
   "source": [
    "### Complex combination rules\n",
    "Let's try giving more weight to the models that are performing better. We already have the models' test-set performance in `scores` so let's use that... \n",
    "\n",
    "But wait! We would be using test data to (ultimately) help predict test data, and invalidate our model. And weighting via the training set performance would just say \"listen to the most overfit model you can find, that guy's a genius.\" \n",
    "\n",
    "This is where the tuning set we left aside comes into play. It gives us a chance to measure how well each component model does on new data, without spoiling the test set. (If we had valid estimates of how well each model does out-of-sample, e.g. from out-of-bag estimates, we could use those to decide weights)\n",
    "\n",
    "If we hadn't left a tuning set aside, we'd have to go back to the very beginning, set aside part of our training set to be the tuning set, and fit the base models to just the reduced training set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brutal-salad",
   "metadata": {},
   "source": [
    "**Speaker note**: point out that we've got the old one-tuning-set problem, and CV can assist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surrounded-object",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get each model's overall performance on the tuning data\n",
    "tuning_scores = [m.score(X_tune, y_tune) for m in models]\n",
    "\n",
    "# divide by the sum so the weights all sum to 1.\n",
    "weights = np.array(tuning_scores)/np.sum(tuning_scores)\n",
    "\n",
    "print(\"First five weights:\", weights[0:5])\n",
    "\n",
    "# # on each tuning observation, apply the weight assigned to each model and predict as 1\n",
    "# # if the weighted majority breaks 50%\n",
    "# # np.dot performs matrix multiplication\n",
    "weighted_predictions = np.dot(predictions_test, weights) > 0.5\n",
    "\n",
    "test_acc = accuracy_score(y_test,weighted_predictions)\n",
    "print(f'Test Acc: {test_acc:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wicked-forward",
   "metadata": {},
   "source": [
    "It turns out that weighting didn't change accuracy much. Though, to be fair, the weights are all basically .01, which is what they would be for a pure majority vote. Although we could spend time finding the right way to convert accuracy scores to weights, what we'd really like is to _learn the weights and threshold that are optimal for correctly classifying new points_. [Where have we heard _that_ before?] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lesbian-catalyst",
   "metadata": {},
   "source": [
    "<div class=\"exercise\"><b>Exercise</b></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hindu-mortality",
   "metadata": {},
   "source": [
    "**Q1.5** Create a logistic regression metamodel with ridge regularization to find weights for each of the 100 original models. You should use cross validation to tune the regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "commercial-evaluation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn the optimal weights and intercept\n",
    "# (intercept is related to whether we need a 50% majority or a 66% majority or an 80% majority)\n",
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "innovative-terrain",
   "metadata": {},
   "source": [
    "**Q1.6** Combine the models using the weights you just found.\n",
    "\n",
    "**Hint:** Don't over think it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "curious-soldier",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use weights and intercept to combine the test data predictions\n",
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aggressive-yacht",
   "metadata": {},
   "source": [
    "**Q1.7** Report the accuracy of the metamodel's predictions. How does it perform compared to the individual estimators and the the previous metamodels? What do you think explains this performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "renewable-wyoming",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate logistic metamodel\n",
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "younger-volunteer",
   "metadata": {},
   "source": [
    "**Answer:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "covered-threshold",
   "metadata": {},
   "source": [
    "### More Information?\n",
    "\n",
    "Up to this point we've been trying to find ways of combining the original models' _class predictions_ to improve our accuracy. But what we'd _really_ like to use are the models' _probability predictions_. This would give us much more information to work with.It would be important to know how confident each model is in its predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "civic-affiliation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_from_model_probas(models, X):\n",
    "    '''\n",
    "    Builds a Dataframes of probability predictions on X from a list of classifiers\n",
    "    '''\n",
    "    model_probas = [{f'model{i}': m.predict_proba(X)[:,1]} for i,m in enumerate(models)]\n",
    "    df = pd.concat((pd.DataFrame(m) for m in model_probas), axis=1)\n",
    "    return df\n",
    "\n",
    "probas_train = df_from_model_probas(models, X_train)\n",
    "probas_tune = df_from_model_probas(models, X_tune)\n",
    "probas_test = df_from_model_probas(models, X_test)\n",
    "\n",
    "probas_tune.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "persistent-raise",
   "metadata": {},
   "source": [
    "**Q1.8** Fit a 2nd logistic metamodel, this time on the probability predictions. How does it perform?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shared-cargo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn the optimal weights and intercept\n",
    "# (intercept is related to whether we need a 50% majority or a 66% majority or an 80% majority)\n",
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "innovative-extension",
   "metadata": {},
   "source": [
    "**Answer:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "threatened-preview",
   "metadata": {},
   "source": [
    "We'll use the probabilities for training our metamodels from here on. To keep things simple we'll rename the dataframes to go back the the `predictions_...` convention. You can always experiment with using the class labels on the rest of the notebook by setting `USE_PROBAS` to False and re-running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "olympic-judgment",
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_PROBAS = True\n",
    "if USE_PROBAS:\n",
    "    predictions_tune = probas_tune\n",
    "    predictions_train = probas_train\n",
    "    predictions_test = probas_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "therapeutic-valuable",
   "metadata": {},
   "source": [
    "**Q1.8** How well does a decision tree do at combining the models? (Be careful about how you pick your parameters. Again, they should be tuned with cross validation)\n",
    "\n",
    "**Hint:** There is no CV version of the decision tree in SKLearn like there is for logistic regression. You'll need to use something like `cross_val_scores` instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gothic-stone",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visible-cache",
   "metadata": {},
   "source": [
    "**Answer:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eleven-philippines",
   "metadata": {},
   "source": [
    "**Return to the main Zoom room** :) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "automated-arkansas",
   "metadata": {},
   "source": [
    "Importantly, any parameter hunting we do for the meta-model needs to be done via data from the tuning set. We're not allowed to peek at the test set, and the training set has already been used to tune and fit the base models. *All meta-parameters are found via the tuning set*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "magnetic-snowboard",
   "metadata": {},
   "source": [
    "**Speaker note**: talk about building the models yourself, e.g. combining well-tuned RF to get a good combined model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legendary-corner",
   "metadata": {},
   "source": [
    "### Summary of Ensembling\n",
    "- Ensemble models can be understood as building a bunch of models on the training data and combining them somehow\n",
    "- Visually, you can imagine a dataset consisting of each model's prediction or (if available) its probability estimate for each data point\n",
    "- The art of ensembling is in building models that complement each other, and picking a rule to combine them\n",
    "- You can even use the data to tell you what the combination rule should be, but you have to use data separate from the training and test sets to learn this rule. Simply combining the models via linear/logistic regression is a popular choice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solved-estate",
   "metadata": {},
   "source": [
    "## Part 2: Bagging & Hyperparameter Tuning\n",
    "You've been working with bootstrapping since the beginning of the course, so you should be a pro when it comes to bagging. Remember that random forests are a generalization of bagged trees.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beneficial-thunder",
   "metadata": {},
   "source": [
    "\n",
    "### Tuning the hyperparameters\n",
    "\n",
    "Random Forests generally perform very well out-of-the-box, with the sklearn's default hyperparameters. Some of the tunable parameters are:\n",
    "- The number of trees in the forest: `n_estimators`, int, default=100\n",
    "- The complexity of each tree: \n",
    "    - stop when a leaf has <= `min_samples_leaf` samples \n",
    "    - the `max_depth` of each tree\n",
    "- The sampling scheme: number of features to consider at any given split: `max_features` {“auto”, “sqrt”, “log2”}, int or float, default=”auto”.\n",
    "\n",
    "#### Hints:\n",
    "\n",
    "<a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\" target=\"_blank\">RandomForestClassifier()</a> : Defines the RandomForestClassifier and includes more details on the definition and range of values for its tunable parameters.\n",
    "\n",
    "<a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier.predict_proba\" target=\"_blank\">model.predict_proba(X)</a> : Predict class probabilities for X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lucky-bowling",
   "metadata": {},
   "source": [
    "### Vanilla random forest\n",
    "\n",
    "We'll start by training a Random Forest Classifier using the default parameters and calculate the test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "binary-notification",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "seed = 0\n",
    "# Define a Random Forest classifier with randon_state = seed\n",
    "vanilla_rf = RandomForestClassifier(random_state=seed) \n",
    "\n",
    "# Fit the model on the entire data\n",
    "vanilla_rf.fit(predictions_tune, y_tune);\n",
    "\n",
    "# Calculate ACC & AUC/ROC on the test set\n",
    "y_proba = vanilla_rf.predict_proba(predictions_test)[:, 1]\n",
    "acc = accuracy_score(y_test, y_proba > 0.5)\n",
    "auc = roc_auc_score(y_test, y_proba)\n",
    "print(f'Vanilla RF ACC on test set:{acc:.3f}')\n",
    "print(f'Vanilla RF AUC on test set:{auc:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecological-brief",
   "metadata": {},
   "source": [
    "### Number of trees, `n_estimators`, default = 100\n",
    "\n",
    "Trees in a RF are called _estimators_.\n",
    "One way to tune our random forest classifer's `n_estimators` hyperparameter is to:\n",
    "1. iterate over candidate values \n",
    "2. instantiate a classifer with current value\n",
    "3. pass the classifer to `cross_val_score`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suspended-mitchell",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "min_estimators = 50\n",
    "max_estimators = 500\n",
    "step_estimators = 50\n",
    "n_estimators_list = np.arange(min_estimators, max_estimators+1, step_estimators)\n",
    "cvmeans = []\n",
    "for n_estimators in n_estimators_list:\n",
    "    clf = RandomForestClassifier(n_estimators=n_estimators, max_depth = 10, max_features='auto', warm_start=True, random_state=seed)\n",
    "    # Perform 5-fold cross validation and store results\n",
    "    scores = cross_val_score(estimator=clf, X=predictions_tune, y=y_tune, cv=5)\n",
    "    cvmeans.append(scores.mean())\n",
    "    print(\"n_estimators =\", n_estimators,\": val_accuracy =\",scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "manufactured-netscape",
   "metadata": {},
   "source": [
    "This can be rather slow, especially if we are using a large number of estimators and/or there are many candidate values we'd like to try."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "silent-religion",
   "metadata": {},
   "source": [
    "<div class=\"discussion\"><b>Discussion</b></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aquatic-wright",
   "metadata": {},
   "source": [
    "**Q2.1**: Why do you think cross validation might be slow with random forests?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "geographic-leather",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distinct-gambling",
   "metadata": {},
   "source": [
    "**Q2.2**: We have this this `predictions_train` dataframe from earlier. Considering what's been said about the tuning data, can you think of any future use for `preductions_train`?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southern-keyboard",
   "metadata": {},
   "source": [
    "**Answer:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "young-laptop",
   "metadata": {},
   "source": [
    "### OOB Error\n",
    "\n",
    "A faster option is to use _out-of-bag error (oob)_ to evaluate each candidate hyperparameter value.\n",
    "\n",
    "Recall that when bootstrapping each estimator in the RF, some observations will not be sampled. Because the estimator wasn't fit on these observations they can be used as a 'free' validation set!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polish-asthma",
   "metadata": {},
   "source": [
    "The number of estimators (i.e., trees) needs to be large enough for the $oob$ error to stabilize in its lowest possible value. A good start is 10 times the number of features, however, adjusting other hyperparameters will influence the optimum number of trees (more on that shortly!). \n",
    "\n",
    "**Note:** \n",
    "* The RF has an `oob_score_` attribute. This is the _oob accuracy_. How would you get the _oob error rate_?\n",
    "* Instantiating the RF classifer inside the loop at each iteration slows things down. We can instead instantiate the RF _before_ the loop and simply change any parameter values we like from inside the loop using the classifier's `set_params()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "focal-disclaimer",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "clf = RandomForestClassifier(warm_start=True, \n",
    "                               oob_score=True,\n",
    "                               min_samples_leaf=40,\n",
    "                               max_depth = 10,\n",
    "                               random_state=seed)\n",
    "\n",
    "error_rate = {}\n",
    "\n",
    "for i in range(min_estimators, max_estimators + 1):\n",
    "    clf.set_params(n_estimators=i) \n",
    "    clf.fit(X_tune, y_tune)\n",
    "\n",
    "    # Record the OOB error for each `n_estimators=i` setting.\n",
    "    oob_error = 1 - clf.oob_score_\n",
    "    error_rate[i] = oob_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loose-chance",
   "metadata": {},
   "source": [
    "**Q2.3**: WHat does the `warm_start` parameter do? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "requested-synthesis",
   "metadata": {},
   "source": [
    "**Answer:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "governing-jacket",
   "metadata": {},
   "source": [
    "Now we can plot the $oob$ error of a random forest as a function of the number of trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "macro-desktop",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the \"OOB error rate\" vs. \"n_estimators\" plot.\n",
    "# OOB error rate = num_missclassified/total observations (%)\\\n",
    "xs = []\n",
    "ys = []\n",
    "for label, clf_err in error_rate.items():\n",
    "    xs.append(label)\n",
    "    ys.append(clf_err)   \n",
    "plt.plot(xs, ys)\n",
    "plt.xlim(min_estimators, max_estimators)\n",
    "plt.xlabel(\"n_estimators\")\n",
    "plt.ylabel(\"OOB error rate\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fallen-witch",
   "metadata": {},
   "source": [
    "Weevaluated just fit and evaluated 50x the number of models in just twice the time!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chubby-saying",
   "metadata": {},
   "source": [
    "### `min_samples_leaf`, default = 1\n",
    "\n",
    "The minimum number of samples required to be at a leaf node. A split point at any depth will only be considered if it leaves at least min_samples_leaf training samples in each of the left and right branches. This may have the effect of smoothing the model, especially in regression. We will plot various values of the `min_samples_leaf` with `num_iterators`. \n",
    "\n",
    "**Note:** This takes about 2 minutes to run. We can use that time to understand the code! :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confidential-mother",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# runtime ~2 minutes\n",
    "from collections import OrderedDict\n",
    "# instantiate the template classifier\n",
    "clf = RandomForestClassifier(oob_score=True,\n",
    "                             max_depth = 10,\n",
    "                             random_state=seed)\n",
    "\n",
    "# set some hyperparameter candidates\n",
    "min_estimators = 100\n",
    "max_estimators = 650\n",
    "min_samples_leaf_list = [1, 5]\n",
    "\n",
    "# Map a label (the value of `min_samples_leaf`) to a list of (model, oob error) tuples.\n",
    "# error_rate = OrderedDict((label, []) for label in min_samples_leaf_list)\n",
    "error_rate = {label: [] for label in min_samples_leaf_list}\n",
    "\n",
    "for min_samples_leaf in min_samples_leaf_list:\n",
    "    clf.set_params(min_samples_leaf=min_samples_leaf) \n",
    "    for i in range(min_estimators, max_estimators + 1):\n",
    "        clf.set_params(n_estimators=i) \n",
    "        clf.fit(X_tune, y_tune)\n",
    "        # Record the OOB error for each model\n",
    "        oob_error = 1 - clf.oob_score_\n",
    "        error_rate[min_samples_leaf].append((i, oob_error))\n",
    "        # warm_start is complicated if we when looping over\n",
    "        # other parameters in addition to n_estimators\n",
    "        clf.set_params(warm_start=True)\n",
    "    clf.set_params(warm_start=False)\n",
    "\n",
    "for label, clf_err in error_rate.items():\n",
    "    xs, ys = zip(*clf_err)\n",
    "    plt.plot(xs, ys, label=f'min_samples_leaf={label}')\n",
    "\n",
    "plt.xlim(min_estimators, max_estimators)\n",
    "plt.xlabel(\"n_estimators\")\n",
    "plt.ylabel(\"OOB error rate\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "settled-basin",
   "metadata": {},
   "source": [
    "We can then look at the lowest error to choose our parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "direct-learning",
   "metadata": {},
   "outputs": [],
   "source": [
    "err = 100\n",
    "best_num_estimators = 0\n",
    "for label, clf_err in error_rate.items():\n",
    "    num_estimators, error = min(clf_err, key=lambda n: (n[1], -n[0]))\n",
    "    if error<err: err=error; best_num_estimators = num_estimators; best_leaf = label\n",
    "\n",
    "print(f'Optimum num of estimators: {best_num_estimators} \\nmin_samples_leaf: {best_leaf}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "static-worst",
   "metadata": {},
   "source": [
    "Now we re-train the Random Forest Classifier using the new values for the parameters and calculate the AUC/ROC. Include another parameter, the `max_features`, the number of features to consider when looking for the best split. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strange-reynolds",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators_rf = RandomForestClassifier(n_estimators= best_num_estimators,\n",
    "                                    random_state=seed,\n",
    "                                    oob_score=True,\n",
    "                                    min_samples_leaf=best_leaf,\n",
    "                                    max_features='sqrt') \n",
    "\n",
    "# Fit the model on the entire data\n",
    "estimators_rf.fit(predictions_tune, y_tune);\n",
    "\n",
    "# # Calculate accuracy\n",
    "# y_pred = estimators_rf.predict(X_test)\n",
    "# acc = np.round(estimators_rf.score(X_test, y_test),2)\n",
    "# print(f'Plain RF accuracy on test set:{acc}')\n",
    "\n",
    "# Calculate AUC/ROC\n",
    "y_proba = estimators_rf.predict_proba(predictions_test)[:, 1]\n",
    "estimators_acc = accuracy_score(y_test, y_proba > 0.5)\n",
    "estimators_auc = np.round(roc_auc_score(y_test, y_proba),2)\n",
    "print(f'Educated RF ACC on test set:{estimators_acc:.3f}')\n",
    "print(f'Educated RF AUC on test set:{estimators_auc:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "timely-howard",
   "metadata": {},
   "source": [
    "Look at the model's parameters with `get_params()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "driving-transparency",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators_rf.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pointed-spare",
   "metadata": {},
   "source": [
    "### GridSearchCV\n",
    "\n",
    "After we have some idea of the range of optimum values for the number of trees and maybe a couple of other parameters, and have enough computing power, you may perform an exhaustive search over other parameter values.\n",
    "\n",
    "<a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\" target=\"_blank\">GridSearchCV()</a> : Performes exhaustive search over specified parameter values for an estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "signal-provincial",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "# not that warm_start isn't useful if we are not tuning num_estimators\n",
    "rf = RandomForestClassifier(n_jobs=-1,\n",
    "                           n_estimators= best_num_estimators,\n",
    "                           oob_score=True,\n",
    "                           min_samples_leaf=best_leaf,\n",
    "                           random_state=seed)\n",
    "\n",
    "param_dict = {\n",
    "    'min_samples_split': [2,5,None],\n",
    "    'max_features': ['sqrt', 'auto']\n",
    "}\n",
    "\n",
    "scoring = {'ACC': make_scorer(accuracy_score), 'AUC': 'roc_auc'}\n",
    "\n",
    "grid_search = GridSearchCV(rf, \n",
    "                           param_dict, \n",
    "                           scoring=scoring, \n",
    "                           refit='ACC', \n",
    "                           return_train_score=True, \n",
    "                           n_jobs=-1)\n",
    "\n",
    "results = grid_search.fit(predictions_tune, y_tune)\n",
    "best_rf = results.best_estimator_\n",
    "display.display(best_rf.get_params())\n",
    "# Calculate AUC/ROC\n",
    "y_proba = best_rf.predict_proba(predictions_test)[:, 1]\n",
    "grid_acc = accuracy_score(y_test, y_proba > 0.5)\n",
    "grid_auc = roc_auc_score(y_test, y_proba)\n",
    "print(f'GridSearchCV RF ACC on test set:{grid_acc:.3f}')\n",
    "print(f'GridSearchCV RF AUC on test set:{grid_auc:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "genetic-shelf",
   "metadata": {},
   "source": [
    "That was pretty quick! But if we wanted to search tune more parameters the number of combinations quickly gets large and we'd really start to notice cross validation slowing us down."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attempted-measure",
   "metadata": {},
   "source": [
    "## Skip the CV with ParameterGrid!\n",
    "\n",
    "[ParameterGrid](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.ParameterGrid.html) creates a generator of dictionaries from your original `param_dict`, one dictionary for each combination of parameters!\n",
    "\n",
    "Now we can just loop over this generator, using `set_params()` and `oob_score_` without having to be slowed down by cross validation. It's the best of both worlds!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cooked-class",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# runtime ~1 minute\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "param_grid = ParameterGrid(param_dict)\n",
    "\n",
    "clf = RandomForestClassifier(n_jobs=-1,\n",
    "                               n_estimators = best_num_estimators,\n",
    "                               oob_score = True,\n",
    "                               warm_start=  True,\n",
    "                               min_samples_leaf=best_leaf,\n",
    "                               random_state=seed)\n",
    "error = 100\n",
    "max_estimators = max([params['n_estimators'] for params in param_grid])\n",
    "best_params = {}\n",
    "for params in param_grid:\n",
    "    clf.set_params(**params)\n",
    "    clf.fit(predictions_tune, y_tune)\n",
    "    # Record the OOB error for each model\n",
    "    oob_error = 1 - clf.oob_score_\n",
    "    if oob_error < error:\n",
    "        best_params = clf.get_params()\n",
    "        if params['n_estimators'] == max_estimators:\n",
    "            clf.set_params(warm_start=False)\n",
    "        else:\n",
    "            clf.set_params(warm_start=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ideal-award",
   "metadata": {},
   "outputs": [],
   "source": [
    "pgrid = clf.set_params(**best_params)\n",
    "y_proba = pgrid.predict_proba(predictions_test)[:,1]\n",
    "pgrid_acc = accuracy_score(y_test, y_proba > 0.5)\n",
    "pgrid_auc = roc_auc_score(y_test, y_proba)\n",
    "print(f'ParamGrid RF ACC on test set:{pgrid_acc:.3f}')\n",
    "print(f'ParamGrid RF AUC on test set:{pgrid_auc:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "otherwise-research",
   "metadata": {},
   "source": [
    "## Part 3: Boosting\n",
    "We wanted to spend a few more words on boosting, since it can sometimes take a second pass to make sense of it.\n",
    "\n",
    "Boosting is a particular way of building and combing the models in an ensemble. Most ensemble models don't care about the order the models are built in, but in boosting we train a sequence of models where each later model tries to do well on the data points that the current team of models isn't getting right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addressed-providence",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(0, 2*np.pi, 0.1)\n",
    "y = np.sin(x) + 0.1*np.random.normal(size=x.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "animated-newport",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "estgb = GradientBoostingRegressor(n_estimators=501, max_depth=1, learning_rate=1)\n",
    "estgb.fit(x.reshape(-1,1), y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reverse-invasion",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "display_iters = [0, 1, 2, 3, 4, 5, 6, 10, 20, 50, 100, 200, 400, 500]\n",
    "\n",
    "def staged_fit_plot(estgb, x, y, display_iters):\n",
    "    # code from http://nbviewer.jupyter.org/github/pprett/pydata-gbrt-tutorial/blob/master/gbrt-tutorial.ipynb\n",
    "    fig, ax = plt.subplots(1,2, figsize=(20,10), sharey=True)\n",
    "    ax[0].plot(x, y, '.');\n",
    "\n",
    "    sleep_time = 2\n",
    "\n",
    "    # the predicitions given by staged_predict skips the intial predict-the-mean model, so put it back\n",
    "    overall_predictions = list(estgb.staged_predict(x.reshape(-1,1)))\n",
    "    overall_predictions = [np.mean(y)*np.ones_like(x)] + overall_predictions\n",
    "\n",
    "    # for various points in the run\n",
    "    for i in display_iters:\n",
    "\n",
    "        # plot the current overall prediction in the left panel\n",
    "        cur_overall_prediction = overall_predictions[i]\n",
    "        ax[0].plot(x, cur_overall_prediction, alpha=0.7, label=str(i), lw=2)\n",
    "        ax[0].legend()\n",
    "\n",
    "        # plot the current residuals in the right panel\n",
    "        resid = y - cur_overall_prediction\n",
    "        ax[1].cla()\n",
    "        ax[1].scatter(x,resid, label=\"Current Residuals\")\n",
    "        ax[1].axhline(0)\n",
    "\n",
    "        # if early, also plot the model fitted to these residuals\n",
    "        if i <=5:\n",
    "            cur_est = estgb.estimators_[i,0]\n",
    "            cur_prediction = cur_est.predict(x.reshape(-1,1))\n",
    "            ax[1].plot(x, cur_prediction, color='orange', label=\"Newest Model\")\n",
    "        else:\n",
    "            # if late, accalerate\n",
    "            sleep_time = sleep_time/2\n",
    "        ax[1].legend()\n",
    "\n",
    "        # plot\n",
    "        display.display(fig)\n",
    "        display.clear_output(wait=True)\n",
    "        time.sleep(sleep_time)\n",
    "\n",
    "staged_fit_plot(estgb, x, y, display_iters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "christian-tobago",
   "metadata": {},
   "source": [
    "In the left panel, we see an animated version of the slides from class: as we run boosting, the overall model fits (and overfits) the data better and better. In the right panel, we can see the residuals left by the current model, and the model fit to them. At each iteration, $\\text{learning_rate}\\cdot\\text{right_panel_model}$ is added to the latest line in the left panel. You can see how the newest model (right panel) determines how much the overall model (left panel) changes, and why Boosting might be connected to derivatives and gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "developed-patch",
   "metadata": {},
   "source": [
    "<div class=\"exercise\"><b>Exercise</b></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modified-measure",
   "metadata": {},
   "source": [
    "**Q3.1** What is the effect does the learning rate <1 have on the overall model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "whole-mailman",
   "metadata": {},
   "source": [
    "**Answer**:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broke-yield",
   "metadata": {},
   "source": [
    "**Q3.2.** What changes when we use a max_depth of 2 or 3?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intensive-tackle",
   "metadata": {},
   "source": [
    "**Answer:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proved-circuit",
   "metadata": {},
   "source": [
    "### Tuning a Boosted Model\n",
    "Now you should try fine tuning a boosted model like the one above using some form of grid search (the `ParameterGrid` method is recommended). Some parameters to play with would be `n_estimators`, `max_depth`, and `learning_rate`. \n",
    "\n",
    "**Q3.3** Save your tuned GradientBoostingRegressor object as `gboost` and store its best parameters as `best_params`\n",
    "\n",
    "**Note:** We've added more data points and more noise to keep things interesting!\n",
    "\n",
    "**Hints:** \n",
    "- You will have to `reshape(-1,1)` the `x` matrix when fitting and predicting, just as in the code above.\n",
    "- Take a look at the [GradientBoostingRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html?highlight=gradientboost#sklearn.ensemble.GradientBoostingRegressor) documentations for more options.\n",
    "- Refer to the RF GridSearch code above for guidance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "announced-favor",
   "metadata": {},
   "outputs": [],
   "source": [
    "NOISE_SCALE = 0.5\n",
    "N_POINTS = 2500\n",
    "x = np.linspace(0, 2*np.pi, N_POINTS)\n",
    "y = np.sin(x) + NOISE_SCALE*np.random.normal(size=x.shape[0])\n",
    "\n",
    "def sorted_split(x, y, test_size):\n",
    "    '''\n",
    "    We want to shuffle our data when we split.\n",
    "    But this can result in spaghetti plots later!\n",
    "    This helper function sorts the results of our splits for us\n",
    "    '''\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=.2)\n",
    "    train_sort_idx = np.argsort(x_train)\n",
    "    x_train = x_train[train_sort_idx]\n",
    "    y_train = y_train[train_sort_idx]\n",
    "    test_sort_idx = np.argsort(x_test)\n",
    "    x_test = x_test[test_sort_idx]\n",
    "    y_test = y_test[test_sort_idx]\n",
    "    return x_train, x_test, y_train, y_test\n",
    "    \n",
    "    \n",
    "x_train, x_test, y_train, y_test = sorted_split(x, y, test_size=.2)\n",
    "x_train, x_val, y_train, y_val = sorted_split(x_train, y_train, test_size=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rapid-drive",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collectible-retail",
   "metadata": {},
   "source": [
    "Let's take a look at `best_params` and make sure `gboost` is using them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recognized-simple",
   "metadata": {},
   "outputs": [],
   "source": [
    "display.display(best_params)\n",
    "gboost.set_params(**best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "otherwise-storage",
   "metadata": {},
   "source": [
    "If you like, you can view the boosting animation on your own tuned model with the cell below. Watch those residuals shrink!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "american-pastor",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = best_params['n_estimators']\n",
    "if n_estimators > max(display_iters):\n",
    "    display_iters = display_iters + np.arange(max(display_iters), n_estimators+1, 100) \n",
    "staged_fit_plot(gboost, x_train, y_train, display_iters=display_iters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demanding-snowboard",
   "metadata": {},
   "source": [
    "How did you do? Are you overfitting? Below you can view the model predictions on train and test side-by-side as well as the accuracy on all 3 datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medical-diving",
   "metadata": {},
   "source": [
    "**Q3.4** Revisit your gridsearch above and tweak the parameters you are searching over. What's the best test score you can achieve? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "civic-dollar",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2, figsize=(20,10), sharey=True)\n",
    "for i, (x, y, title) in enumerate(zip([x_train, x_test], [y_train, y_test], ['Train', 'Test'])):\n",
    "    ax[i].plot(x, y, '.');\n",
    "    ax[i].plot(x, gboost.predict(x.reshape(-1,1)), alpha=0.7, label=str(i), lw=2)\n",
    "    ax[i].set_title(f'Gradient Boost on {title} Data')\n",
    "    ax[i].legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "needed-amateur",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how high can you get your test score?\n",
    "print('Train R^2:', gboost.score(x_train.reshape(-1,1), y_train)) \n",
    "print('Val   R^2:', gboost.score(x_val.reshape(-1,1), y_val))\n",
    "print('Test  R^2:', gboost.score(x_test.reshape(-1,1),y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "magnetic-montgomery",
   "metadata": {},
   "source": [
    "**Please return to the main Zoom room :)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comic-ladder",
   "metadata": {},
   "source": [
    "### Summary of Bagging and Boosting\n",
    "- Bagging and boosting both build ensemble models in special ways\n",
    "  - Thus, they each have a specific way of building new models, and a speciefic way of combining them\n",
    "  - The homework has you explore thier relationship much more deeply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unknown-synthetic",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
