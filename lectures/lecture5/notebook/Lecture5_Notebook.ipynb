{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <img style=\"float: left; padding-right: 10px; width: 45px\" src=\"https://github.com/Harvard-IACS/2021-s109a/blob/master/lectures/crest.png?raw=true\"> CS-S109A Introduction to Data Science \n",
    "\n",
    "## Lecture 5: Cross-Validation and Regularization\n",
    "\n",
    "**Harvard University**<br>\n",
    "**Summer 2021**<br>\n",
    "**Instructors:** Kevin Rader<br>\n",
    "**Authors:** Rahul Dave, David Sondak, Will Claybaugh, Pavlos Protopapas, Chris Tanner, Kevin Rader\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RUN THIS CELL TO GET THE RIGHT FORMATTING \n",
    "import requests\n",
    "from IPython.core.display import HTML\n",
    "styles = requests.get(\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/cs109.css\").text\n",
    "HTML(styles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents \n",
    "<ol start=\"0\">\n",
    "<li> Learning Goals </li>\n",
    "<li> Cross-Validation</li>\n",
    "<li> Regularization</li>\n",
    "<li> Tuning Hyperparameters</li>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Goals\n",
    "\n",
    "This Jupyter notebook accompanies Lecture 5. By the end of this lecture, you should be able to:\n",
    "\n",
    "- Know how to manually perform random subsets cross-validation\n",
    "- Perform $k$-fold cross-v alidation both manually and using provided libraries\n",
    "- Perform both forms of regularization (Ridge and LASSO) and understand the most basic use differences\n",
    "- Tune the 'shrinkage' parameter on \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sklearn as sk\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "import statsmodels as sm\n",
    "import statsmodels.regression.linear_model as lm\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "pd.set_option('display.width', 500)\n",
    "pd.set_option('display.max_columns', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0: Reading the data \n",
    "\n",
    "We will be using the same data as last week (both the train and test splits): modeling `votergap` from the 2020 election (Biden-Trump) from many predictors in the `county_election_2020` dataset.\n",
    "\n",
    "We start by reading in the datasets for you and refitting some regression models from last week:\n",
    "\n",
    "**As always: use the training dataset for all exploratory analysis and model fitting.  Only use the test dataset to evaluate models.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train = pd.read_csv(\"../data/county_election_train.csv\")\n",
    "test = pd.read_csv(\"../data/county_election_test.csv\")\n",
    "\n",
    "#creating Y\n",
    "votergap_train = 100*(train['biden_perc']-train['trump_perc'])\n",
    "votergap_test = 100*(test['biden_perc']-test['trump_perc'])\n",
    "\n",
    "train['votergap'] = votergap_train\n",
    "test['votergap'] = votergap_test\n",
    "\n",
    "#recall we log-transformed several of the variables and did a little imputation in cancer variable (with the median)\n",
    "train['log_density'] = np.float64(np.log(train['density']))\n",
    "train['log_minority'] = np.float64(np.log(train['minority']))\n",
    "train['log_population'] = np.log(train['population'])\n",
    "train['log_hispanic'] = np.log(train['hispanic'])\n",
    "\n",
    "test['log_density'] = np.float64(np.log(test['density']))\n",
    "test['log_minority'] = np.float64(np.log(test['minority']))\n",
    "test['log_population'] = np.log(test['population'])\n",
    "test['log_hispanic'] = np.log(test['hispanic'])\n",
    "\n",
    "# imputing median cancer rate for the 40 or so counties with missing cancer rates\n",
    "train['cancer'].loc[train['cancer'].isnull()] = np.median(train['cancer'])\n",
    "test['cancer'].loc[test['cancer'].isnull()] = np.median(train['cancer'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q0.1** How were missing values imputed for the variable `cancer` in the test set?  Why was this choice made?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and we fit the simple regression model for y = votergap and x = log_density in both sklearn and statmodels\n",
    "\n",
    "predictors = ['log_population','log_hispanic', 'log_minority', 'female', \n",
    "             'unemployed', 'income', 'nodegree', 'bachelor', 'inactivity', 'obesity', 'log_density']\n",
    "X_train = train[predictors]\n",
    "X_test = test[predictors]\n",
    "\n",
    "regress_maineffects = LinearRegression(fit_intercept=True).fit(X_train, train['votergap'])\n",
    "\n",
    "r2_test_maineffects = sk.metrics.r2_score(test['votergap'], regress_maineffects.predict(X_test))\n",
    "print(\"R-squared on the test set for the main effects model:\", r2_test_maineffects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And then we fit a model to all the 2-way interaciton terms\n",
    "\n",
    "X_interact = PolynomialFeatures(2, interaction_only=False, include_bias=True).fit_transform(train[predictors])\n",
    "print(\"There are\", X_interact.shape[1]-train[predictors].shape[1]-1,\"interaction terms in the design matrix for this model.\")\n",
    "\n",
    "regress_interact = LinearRegression(fit_intercept=False).fit(X_interact, train['votergap'])\n",
    "\n",
    "r2_test_interact = sk.metrics.r2_score(test['votergap'], regress_interact.predict(\n",
    "    PolynomialFeatures(2, interaction_only=False, include_bias=True).fit_transform(test[predictors])))\n",
    "print(\"R-squared on the test set for the full interaction model:\", r2_test_interact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And lastly we did variable selection on this full model with interaction (backwards sequential variable selection)\n",
    "\n",
    "df_backwards = pd.DataFrame(X_interact)\n",
    "cutoff = 0.05\n",
    "\n",
    "for i in np.arange(df_backwards.shape[1]):\n",
    "    model_temp = lm.OLS(train['votergap'],df_backwards).fit()\n",
    "    if(np.max(model_temp.pvalues) > cutoff):\n",
    "        print(\"Predictor#:\", np.argmax(model_temp.pvalues), \"with associated p-value of\" ,np.max(model_temp.pvalues))\n",
    "        df_backwards = df_backwards.drop(df_backwards.columns[np.argmax(model_temp.pvalues)],axis=1)\n",
    "    else:\n",
    "        break\n",
    "\n",
    "regress_backwards = LinearRegression(fit_intercept=False).fit(df_backwards, train['votergap'])\n",
    "df_backwards.shape\n",
    "\n",
    "X_interact_test = PolynomialFeatures(2, interaction_only=False, include_bias=True).fit_transform(test[predictors])\n",
    "X_backwards_test = pd.DataFrame(X_interact_test)[df_backwards.columns]\n",
    "\n",
    "r2_test_backwards = sk.metrics.r2_score(test['votergap'], regress_backwards.predict(X_backwards_test))\n",
    "print(\"R-squared on the test set for the backwards-selected model:\", r2_test_backwards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q0.2** Play around with the critical p-value `cut-off` for keeping variables.  What does this say about using 0.05 as the cut-off?  Will this generalize to all situations?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Basic Cross-Validation\n",
    "\n",
    "3 models were built above, and a single test-set was used to determine which was best for out-of-sample prediction.  Let's instead use cross-validation to decide which of the 3 models is best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.RandomState(109)\n",
    "\n",
    "n_folds = 10\n",
    "k_fold = KFold(n_folds, random_state=109, shuffle=True)\n",
    "y_train = train['votergap']\n",
    "X_backwards = df_backwards\n",
    "X_interact = pd.DataFrame(X_interact)\n",
    "\n",
    "r2_cv_maineffects = []\n",
    "r2_cv_interact = []\n",
    "r2_cv_backwards = []\n",
    "\n",
    "for traincv_index, testcv_index in k_fold.split(X_train):\n",
    "    y_traincv, y_testcv = y_train[traincv_index], y_train[testcv_index]\n",
    "        \n",
    "    X_maineffects_traincv, X_maineffects_testcv = X_train.loc[traincv_index], X_train.loc[testcv_index]\n",
    "    X_interact_traincv, X_interact_testcv = X_interact.loc[traincv_index], X_interact.loc[testcv_index]\n",
    "    X_backwards_traincv, X_backwards_testcv = X_backwards.loc[traincv_index], X_backwards.loc[testcv_index]\n",
    "\n",
    "    model1 = LinearRegression(fit_intercept=True).fit(X_maineffects_traincv, y_traincv)\n",
    "    model2 = LinearRegression(fit_intercept=True).fit(X_interact_traincv, y_traincv)\n",
    "    model3 = LinearRegression(fit_intercept=True).fit(X_backwards_traincv, y_traincv)\n",
    "\n",
    "    r2_cv_maineffects.append(sk.metrics.r2_score(y_testcv, model1.predict(X_maineffects_testcv)))\n",
    "    r2_cv_interact.append(sk.metrics.r2_score(y_testcv, model2.predict(X_interact_testcv)))\n",
    "    r2_cv_backwards.append(sk.metrics.r2_score(y_testcv, model3.predict(X_backwards_testcv)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1.1** Determine which of the 3 models considered above are best for out-of-sample prediction (both numerically and visually) based on their $R^2$ scores in the validation sets.  Does this agree with using the original test set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######\n",
    "# your code here\n",
    "######\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1.2** Play around with $k$ in the $k$-fold cross-validation above.  Does the choice of the number of folds really matter?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######\n",
    "# your code here\n",
    "######\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "*your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1.3** Build your own bootstrap cross-validation approach below (a skeleton is provided).  Do you see the same result comparing these 3 models?  What are the advantages to $k$-fold and random subsets/bootstrap cross-validation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nboots = 100\n",
    "n_train = train.shape[0]\n",
    "traincv_size = 0.8\n",
    "n_traincv = int(traincv_size*n_train)\n",
    "\n",
    "for boot in np.arange(nboots):\n",
    "    traincv_indices = np.random.choice(n_train,n_traincv,replace=False)\n",
    "    traincv_index = random_indices[0:n_traincv]\n",
    "    testcv_index = random_indices[n_traincv:n_train]\n",
    "    \n",
    "    ######\n",
    "    # your code here\n",
    "    ######\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Data augmentation\n",
    "\n",
    "In this section, we will create two different design matrices that will be used going forward for applying reularization to.  The first is a high-order (10) polynomial regression using `log-minority` only (done for you below), and the second is a high-dimensional design matrix considering all factors (in **Q2.2**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_minority_poly = PolynomialFeatures(10, include_bias=False).fit_transform(train[['log_minority']])\n",
    "\n",
    "regress_minority_poly = LinearRegression(fit_intercept=True).fit(X_minority_poly , train['votergap'])\n",
    "print(\"Beta0 =\", regress_minority_poly.intercept_ ,\", Beta1 =\", regress_minority_poly.coef_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2.1** Plot the predicted curves on top of the scatterplot of `votergap` vs. `log_minority` (the notebook from lecture 4 might be helpful for this).  What do you notice?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######\n",
    "# your code here\n",
    "######\n",
    "\n",
    "x_dummy = np.arange(np.min(train['log_minority']),np.max(train['log_minority']),0.1)\n",
    "x_dummy_poly = PolynomialFeatures(10, include_bias=False).fit_transform(pd.DataFrame(x_dummy))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2.2** Create a design matrix (call it `X_over`) with all the interactions and 4th-order polynomials based on the original list of 11 predictors: \n",
    "\n",
    "predictors = ['log_population','log_hispanic', 'log_minority', 'female', \n",
    "             'unemployed', 'income', 'nodegree', 'bachelor', 'inactivity', 'obesity', 'log_density']\n",
    "\n",
    "How many predictors does this create?  \n",
    "\n",
    "**Bonus Question:** Justify this number mathematically (it is not easy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######\n",
    "# your code here: create X_over \n",
    "######\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2.3** Fit the linear model using the over-parameterized design matrix above.  How does it perform on the test set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######\n",
    "# your code here\n",
    "######"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Regularization: Ridge and LASSO\n",
    "\n",
    "In this part we will explore the effects of regularization on linear regression modeling.  First using the 10th order polynomial version of log_minority, then using the over-parameterized design matrix.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q3.1** Fit 3 separate Ridge models with $\\alpha = [1,10,100]$, and 3 searate LASSO models with $\\alpha = [1,10,100]$ using the `X_minority_poly` design matrix.  Compare the estimated coefficients for all 7 models (including the unregularized model)...a well-formatted table is provided below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# we give the first one to you\n",
    "ridge_alpha1 = Ridge(alpha=1,fit_intercept=True).fit(X_minority_poly , train['votergap'])\n",
    "\n",
    "\n",
    "######\n",
    "# your code here\n",
    "######\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We give you the code for a nice table and plot (plot is in the next cell)\n",
    "pd.DataFrame(np.transpose([regress_minority_poly.coef_,ridge_alpha1.coef_,ridge_alpha10.coef_,ridge_alpha100.coef_,\n",
    "                          lasso_alpha1.coef_,lasso_alpha10.coef_,lasso_alpha100.coef_]),\n",
    "                columns = [\"ols\",\"ridge1\",\"ridge10\",\"ridge100\",\"lasso1\",\"lasso10\",\"lasso100\",])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, (ax1, ax2) = plt.subplots(1, 2, sharey=True, figsize = (12,6))\n",
    "\n",
    "ax1.plot([1,10,100],[ridge_alpha1.coef_,ridge_alpha10.coef_,ridge_alpha100.coef_],ls=\"--\")\n",
    "ax1.set_title('Ridge')\n",
    "ax2.plot([1,10,100],[lasso_alpha1.coef_,lasso_alpha10.coef_,lasso_alpha100.coef_],ls=\"-.\")\n",
    "ax2.set_title('Lasso')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q3.2**  Plot the predictions (on top of the scatterplot) for all 7 models.  What do you notice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######\n",
    "# your code here\n",
    "######\n",
    "\n",
    "x_dummy = np.arange(np.min(train['log_minority']),np.max(train['log_minority']),0.1)\n",
    "x_dummy_poly = PolynomialFeatures(10, include_bias=False).fit_transform(pd.DataFrame(x_dummy))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q3.3** For Ridge, *tune* the penalty term using the test set (for `alphas = np.exp(np.arange(-10,10,1))`) for the model using the design matrix defined below.  Which value did you choose? How does this perform on the test set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######\n",
    "# your code here\n",
    "######\n",
    "\n",
    "alphas = np.exp(np.arange(-10,10,1))\n",
    "r2_ridges_test = []\n",
    "X_minority_poly_test = PolynomialFeatures(10, include_bias=False).fit_transform(test[['log_minority']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Ridge and LASSO with CV\n",
    "\n",
    "In this part we will use cross-validation to *tune* the penalization term ($\\alpha$) in both ridge and LASSO regressions for the "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q4.1** Use [ridgeCV](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeCV.html) and [LassoCV](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoCV.html) to tune the models using 5-fold cross-validation for the 10$^{th}$ order polynomial of `log_minority`.  You may need to play around with the values of $\\alpha$ you should consider. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######\n",
    "# your code here\n",
    "######"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q4.2** How do the best choices of $\\alpha$ compare for Ridge and Lasso?  Is this surprising, mathematically?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q4.3** Why is it important to not include the bias term when creating the design matrices?  Why is it important to normalize the predictors?  Were these precautions taken here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q4.4** Fit a well-tuned regularization model on the overly parameterized design matrix `X_over` (let's use `normalize=True` now).  How does this perform on the test set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######\n",
    "# your code here\n",
    "######"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
