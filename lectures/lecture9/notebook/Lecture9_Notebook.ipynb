{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <img style=\"float: left; padding-right: 10px; width: 45px\" src=\"https://github.com/Harvard-IACS/2021-s109a/blob/master/lectures/crest.png?raw=true\"> CS-S109A Introduction to Data Science \n",
    "\n",
    "## Lecture 9: Boosting and Stacking\n",
    "\n",
    "**Harvard University**<br>\n",
    "**Summer 2021**<br>\n",
    "**Instructors:** Kevin Rader<br>\n",
    "**Authors:** Pavlos Protopapas, Chris Gumb, Kevin Rader\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents \n",
    "<ol start=\"0\">\n",
    "<li> Learning Goals </li>\n",
    "<li> Boosted Regression Trees </li> \n",
    "<li> AdaBoost Classification </li> \n",
    "\n",
    "   \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Goals\n",
    "\n",
    "This Jupyter notebook accompanies Lecture 9. By the end of this notebook, you should be able to:\n",
    "\n",
    "- Fit and interpret Boosted Regression Modeling using BaggingRegression and  \n",
    "- Have a general sense of the difference between the 3 tree based methods.\n",
    "- Tune the hyperparameters of a random forest.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Import necessary libraries\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd \n",
    "import itertools\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "# from helper import plot_decision_boundary\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "pd.set_option('display.width', 100)\n",
    "pd.set_option('display.max_columns', 20)\n",
    "plt.rcParams[\"figure.figsize\"] = (12,8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0: Reading and Exploring the data \n",
    "\n",
    "We will be using a small air quality data set to see what factors are related to the Ozone level (really just temperature for pedagaoical simplification).  THe measurements were collected in New York City, from May 1, 1973 to September 30, 1973.\n",
    "\n",
    "We start by reading in the dataset for you and visualizing the main variables: `Ozone` and `Temp`:\n",
    "\n",
    "**Important note: we aren't bothering to split into train, validation, and test...though there is a test set we did not provide.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the dataset\n",
    "df = pd.read_csv(\"../data/airquality.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Take a quick look at the data\n",
    "df = df[df.Ozone.notna()]\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign \"x\" column as the predictor variable and \"y\" as the\n",
    "# We only use Ozone as a predictor for this exercise \n",
    "y,x = df['Ozone'].values,df['Temp'].values\n",
    "\n",
    "# Fancy way of sorting on X - We do this now because we will not split\n",
    "# the data into train/val/test in this exercise\n",
    "x,y = list(zip(*sorted(zip(x,y))))\n",
    "x,y = np.array(x).reshape(-1,1),np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-0f1ca77baa9e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0max2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0max3\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfigsize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m18\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0max1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Ozone'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0max1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_title\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Ozone\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "fig,(ax1,ax2,ax3) = plt.subplots(1,3,figsize = (18,5))\n",
    "\n",
    "\n",
    "ax1.hist(df['Ozone'])\n",
    "ax1.set_title(\"Ozone\")\n",
    "ax2.hist(df['Temp'])\n",
    "ax2.set_title(\"Temp\")\n",
    "ax3.scatter(df['Temp'],df['Ozone'])\n",
    "ax3.set_title(\"y = Ozone vs. x = Temp\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Gradient Boosting Regression Trees\n",
    "\n",
    "Below we start to build a boosted regression model by hand for you to predict `Ozone` from `Temp`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a single decision tree stump \n",
    "basemodel = DecisionTreeRegressor(max_depth=1)\n",
    "basemodel.fit(x, y)\n",
    "y_pred = basemodel.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "xrange = np.linspace(x.min(),x.max(),100)\n",
    "plt.plot(x,y,'o',color='#EFAEA4', markersize=6, label=\"True Data\")\n",
    "# plt.plot([x.min(),x.max()],[0,0],'--')\n",
    "plt.xlim()\n",
    "plt.plot(x,y_pred,alpha=0.7,linewidth=3,color='#50AEA4', label='First Tree')\n",
    "plt.xlabel(\"Temperature\", fontsize=16)\n",
    "plt.ylabel(\"Ozone\", fontsize=16)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.legend(loc='best',fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate error residuals\n",
    "residuals = y - y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the residuals to the plot\n",
    "plt.figure(figsize=(10,6))\n",
    "\n",
    "plt.plot(x,y,'o',color='#EFAEA4', markersize=6, label=\"True Data\")\n",
    "plt.plot(x,residuals,'.-',color='#faa0a6', markersize=6, label=\"Residuals\")\n",
    "plt.plot([x.min(),x.max()],[0,0],'--')\n",
    "plt.xlim()\n",
    "plt.plot(x,y_pred,alpha=0.7,linewidth=3,color='#50AEA4', label='First Tree')\n",
    "plt.ylabel(\"Ozone\", fontsize=16)\n",
    "plt.xlabel(\"Temperature\", fontsize=16)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.legend(loc='upper right',fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### edTest(test_fitted_residuals) ###\n",
    "# fit another tree stump on the residuals\n",
    "dtr = DecisionTreeRegressor(max_depth=1);\n",
    "dtr.fit(x, residuals)\n",
    "\n",
    "y_pred_residuals = dtr.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the fit of the residuals to the original plot \n",
    "plt.figure(figsize=(10,6))\n",
    "\n",
    "plt.plot(x,y,'o',color='#EFAEA4', markersize=6, label=\"True Data\")\n",
    "plt.plot(x,residuals,'.-',color='#faa0a6', markersize=6, label=\"Residuals\")\n",
    "plt.plot([x.min(),x.max()],[0,0],'--')\n",
    "plt.xlim()\n",
    "plt.plot(x,y_pred,alpha=0.7,linewidth=3,color='#50AEA4', label='First Tree')\n",
    "plt.plot(x,y_pred_residuals,alpha=0.7,linewidth=3,color='red', label='Residual Tree')\n",
    "plt.ylabel(\"Ozone\", fontsize=16)\n",
    "plt.xlabel(\"Temperature\", fontsize=16)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.legend(loc='center right',fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_ = 0.5\n",
    "y_pred_new = y_pred + lambda_*y_pred_residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "\n",
    "plt.plot(x,y,'o',color='#EFAEA4', markersize=6, label=\"True Data\")\n",
    "plt.plot(x,residuals,'.-',color='#faa0a6', markersize=6, label=\"Residuals\")\n",
    "plt.plot([x.min(),x.max()],[0,0],'--')\n",
    "plt.xlim()\n",
    "plt.plot(x,y_pred,alpha=0.7,linewidth=3,color='#50AEA4', label='First Tree')\n",
    "plt.plot(x,y_pred_residuals,alpha=0.7,linewidth=3,color='red', label='Residual Tree')\n",
    "plt.plot(x,y_pred_new,alpha=0.7,linewidth=3,color='k', label='Boosted Tree')\n",
    "plt.ylabel(\"Ozone\", fontsize=16)\n",
    "plt.xlabel(\"Temperature\", fontsize=16)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.legend(loc='center right',fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1.1** You can continue doing this! Try at least one more interation and play around with the lambda learning rate.  What happens as the iterations increase?  What happens as lambda changes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison with Bagging\n",
    "\n",
    "To compare the two methods, we will be using sklearn's methods and not our own implementation from above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test sets with train size as 0.8 \n",
    "# and random_state as 102\n",
    "# The default value for shuffle is True for train_test_split, so the ordering we \n",
    "# did above is not a problem. \n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=0.8, random_state=109)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a learning rate\n",
    "l_rate = 0.01\n",
    "\n",
    "#  Boosting. We will use the sklearn's boosting model \n",
    "boosted_model  = GradientBoostingRegressor(n_estimators=1000,max_depth=1,learning_rate=l_rate )\n",
    "\n",
    "boosted_model.fit(x_train,y_train)\n",
    "\n",
    "y_pred = boosted_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a bagging model\n",
    "# Specify the number of bootstraps\n",
    "num_bootstraps = 30\n",
    "\n",
    "# Specify the maximum depth of the decision tree\n",
    "max_depth = 100\n",
    "\n",
    "# Define the Bagging Regressor Model\n",
    "# Use Decision Tree as your base estimator with depth as mentioned in max_depth\n",
    "# Initialise number of estimators using the num_bootstraps value\n",
    "# Set max_samples as 0.8\n",
    "bagging_model = BaggingRegressor(DecisionTreeRegressor(max_depth=max_depth),\n",
    "                         n_estimators=num_bootstraps, max_samples=0.8,\n",
    "                        random_state=3)\n",
    "                        \n",
    "\n",
    "# Fit the model on the entire data\n",
    "bagging_model.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred_bagging = bagging_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "\n",
    "xrange = np.linspace(x.min(),x.max(),100).reshape(-1,1)\n",
    "y_pred_boost = boosted_model.predict(xrange)\n",
    "y_pred_bag = bagging_model.predict(xrange)\n",
    "plt.plot(x,y,'o',color='#EFAEA4', markersize=6, label=\"True Data\")\n",
    "plt.xlim()\n",
    "plt.plot(xrange,y_pred_boost,alpha=0.7,linewidth=3,color='#77c2fc', label='Boosting')\n",
    "plt.plot(xrange,y_pred_bag,alpha=0.7,linewidth=3,color='#50AEA4', label='Bagging')\n",
    "plt.xlabel(\"Ozone\", fontsize=16)\n",
    "plt.ylabel(\"Temperature\", fontsize=16)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.legend(loc='best',fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the MSE of the model prediction\n",
    "\n",
    "print(\"The MSE of the Boosting model is\",\n",
    "    round(mean_squared_error(y_test,boosted_model.predict(x_test)),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the MSE of the model prediction\n",
    "\n",
    "print(\"The MSE of the Bagging model is\",\n",
    "    round(mean_squared_error(y_test, y_pred_bagging),3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1.2** What do you notice here?  Why is this not a fair comparison?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1.3** Go back and play with the `learning rate`,`n_estimators` and `max_depth` for Boosting and `n_estimators` and `max_depth` for Bagging.  What do you notice?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1.4** How would RF compare? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can plot the progression of the accuracy/error metric (MSE here) as a funciton of the numbor of trees using the `staged_predict` method for the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "test_score = np.zeros((num_trees,), dtype=np.float64)\n",
    "\n",
    "for i, y_pred in enumerate(boosted_model.staged_predict(x_test)):\n",
    "    test_score[i] = mean_squared_error(y_test, y_pred)\n",
    "    \n",
    "\n",
    "plt.plot(test_score);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1.5** What do you notice in the plot above?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.plot(list(boosted_model.staged_score(x_test,y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: AdaBoost Classification\n",
    "\n",
    "We have a synthetic toy dataset for the classification modeling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the dataset as a pandas dataframe\n",
    "df = pd.read_csv(\"../data/boostingclassifier2.csv\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Read the columns latitude and longitude as the predictor variables\n",
    "X = df[['latitude','longitude']].values\n",
    "\n",
    "# Landtype is the response variable\n",
    "y = df['landtype'].values\n",
    "\n",
    "print(df.shape)\n",
    "\n",
    "X, X_test, y, y_test = train_test_split(X, y, train_size=0.5, random_state=109)\n",
    "\n",
    "print(X.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper plotting function\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "def plot_decision_boundary(classifier, X, y, N = 10, scatter_weights = np.ones(len(y)) , \n",
    "                           ax = None,counter=0,label=False):\n",
    "    '''Utility function to plot decision boundary and scatter plot of data'''\n",
    "    x_min, x_max = X[:, 0].min() - .1, X[:, 0].max() + .1\n",
    "    y_min, y_max = X[:, 1].min() - .1, X[:, 1].max() + .1\n",
    "    xx, yy = np.meshgrid( np.linspace(x_min, x_max, N), np.linspace(y_min, y_max, N))\n",
    "    cmap = ListedColormap([\"#ABCCE3\",\"#50AEA4\"])\n",
    "    \n",
    "    #Check what methods are available\n",
    "    if hasattr(classifier, \"decision_function\"):\n",
    "        zz = np.array( [classifier.decision_function(np.array([xi,yi]).reshape(1,-1)) for  xi, yi in zip(np.ravel(xx), np.ravel(yy)) ] )\n",
    "    elif hasattr(classifier, \"predict_proba\"):\n",
    "        zz = np.array( [classifier.predict_proba(np.array([xi,yi]).reshape(1,-1))[:,1] for  xi, yi in zip(np.ravel(xx), np.ravel(yy)) ] )\n",
    "    else :\n",
    "        zz = np.array( [classifier(np.array([xi,yi]).reshape(1,-1)) for  xi, yi in zip(np.ravel(xx), np.ravel(yy)) ] )\n",
    "            \n",
    "    # reshape result and plot\n",
    "    Z = zz.reshape(xx.shape)\n",
    "    cm_bright = ListedColormap([\"#EFAEA4\",\"#F6345E\"])\n",
    "    \n",
    "    #Get current axis and plot\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    ax.contourf(xx, yy, Z,cmap=cmap,alpha=0.4)\n",
    "    if label:\n",
    "        ax.scatter(X[:,0],X[:,1], c = y, cmap = cm_bright, s = scatter_weights * 40, edgecolors='k',label = f'Stump {counter}')\n",
    "        ax.legend(fontsize=16)\n",
    "    else:\n",
    "        ax.scatter(X[:,0],X[:,1], c = y, cmap = cm_bright, s = scatter_weights * 40, edgecolors='k',label = f'Stump {counter}')\n",
    "    ax.set_xlabel('$Latitude$', fontsize=14)\n",
    "    ax.set_ylabel('$Longitude$', fontsize=14)\n",
    "    ax.set_title(f'Stump {counter+1} decision boundary',fontsize=16)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AdaBoost algorithm implementation from scratch\n",
    "\n",
    "def AdaBoost_scratch(X,y, M=10, learning_rate = 1):\n",
    "    #Initialization of utility variables\n",
    "    N = len(y)\n",
    "    estimator_list, y_predict_list, estimator_error_list, estimator_weight_list, sample_weight_list = [], [],[],[],[]\n",
    "\n",
    "    #Initialize the sample weights\n",
    "    sample_weight = np.ones(N) / N\n",
    "    \n",
    "    # Cooy the sample weights to another list\n",
    "    sample_weight_list.append(sample_weight.copy())\n",
    "\n",
    "    #For m = 1 to M where M is the number of stumps\n",
    "    for m in range(M):   \n",
    "\n",
    "        #Fit a Decision Tree classifier stump with a maximum of 2 leaf nodes\n",
    "        estimator = DecisionTreeClassifier(max_depth = 1, max_leaf_nodes=2)\n",
    "        \n",
    "        # Fit the model on the entire data with the sample weight initialise before\n",
    "        estimator.fit(X, y, sample_weight=sample_weight)\n",
    "        \n",
    "        # Predcit on the entire data\n",
    "        y_predict = estimator.predict(X)\n",
    "\n",
    "        # Compute the number of misclassifications\n",
    "        incorrect = (y_predict != y)\n",
    "\n",
    "        # Compute the error as the mean of the weighted sum of the number of incorrect predictions given the sample weight\n",
    "        estimator_error = np.mean( np.average(incorrect, weights=sample_weight, axis=0))\n",
    "        \n",
    "        # Compute the new weights using the learning rate and estimator error\n",
    "        estimator_weight =  learning_rate * np.log((1. - estimator_error) / estimator_error)\n",
    "\n",
    "        # Boost the sample weights\n",
    "        sample_weight *= np.exp(estimator_weight * incorrect * ((sample_weight > 0) | (estimator_weight < 0)))\n",
    "\n",
    "        # Save the iteration values\n",
    "        estimator_list.append(estimator)\n",
    "        y_predict_list.append(y_predict.copy())\n",
    "        estimator_error_list.append(estimator_error.copy())\n",
    "        estimator_weight_list.append(estimator_weight.copy())\n",
    "        sample_weight_list.append(sample_weight.copy())\n",
    "        \n",
    "\n",
    "    #Convert to np array for convenience   \n",
    "    estimator_list = np.asarray(estimator_list)\n",
    "    y_predict_list = np.asarray(y_predict_list)\n",
    "    estimator_error_list = np.asarray(estimator_error_list)\n",
    "    estimator_weight_list = np.asarray(estimator_weight_list)\n",
    "    sample_weight_list = np.asarray(sample_weight_list)\n",
    "\n",
    "    # Compute the predictions\n",
    "    preds = (np.array([np.sign((y_predict_list[:,point] * estimator_weight_list).sum()) for point in range(N)]))\n",
    "    \n",
    "    # Return the model, estimated weights and sample weights\n",
    "    return estimator_list, estimator_weight_list, sample_weight_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Call the AdaBoost function to perform boosting classification\n",
    "estimator_list, estimator_weight_list, sample_weight_list  = AdaBoost_scratch(X, y, M = 6, learning_rate = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper code to plot the AdaBoost Decision Boundary stumps\n",
    "fig = plt.figure(figsize = (14,14))\n",
    "for m in range(0,6):\n",
    "    fig.add_subplot(3,2,m+1)\n",
    "\n",
    "    s_weights = (sample_weight_list[m,:] / sample_weight_list[m,:].sum() ) * 300\n",
    "    plot_decision_boundary(estimator_list[m], X,y,N = 50, scatter_weights =s_weights,counter=m)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use sklearn's AdaBoostClassifier to take a look at the final decision boundary \n",
    "\n",
    "# Initialise the model with Decision Tree classifier as the base model same as above\n",
    "# Use SAMME as the algorithm and 6 estimators with learning rate as 1\n",
    "boost = AdaBoostClassifier( base_estimator = DecisionTreeClassifier(max_depth = 1, max_leaf_nodes=2), \n",
    "                            algorithm = 'SAMME', n_estimators=6, learning_rate=1.0)\n",
    "\n",
    "# Fit on the entire data\n",
    "boost.fit(X,y)\n",
    "\n",
    "# Call the plot_decision_boundary function to plot the decision boundary of the model \n",
    "plot_decision_boundary(boost, X, y, N = 50)\n",
    "\n",
    "plt.title('AdaBoost Decision Boundary', fontsize=16)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2.1** Play around with the `learning_rate` and the `num_estimators` and see how it affects the boosted trees.  How did they compare?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2.2** Be a little more careful about evaluating the different choices of `learning_rate` and `num_estimators`: for `learning_rate [0.25,0.5,0.75,1]`, use the `list(staged_score)` ([link](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html#sklearn.ensemble.AdaBoostClassifier.staged_score)) method to compare the models accuracy on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########\n",
    "# Your code here\n",
    "#########\n",
    "\n",
    "learns = [0.25,0.5,0.75,1]\n",
    "\n",
    "for learn in learns:\n",
    "    # fit model, \n",
    "    # predict on test using list(staged_score)\n",
    "    # plot the results as a function of # trees\n",
    "    \n",
    "plt.legend();\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2.3** Interpret the plot above.  What can you conclude from this toy example?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
